{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoNLL-2003 Example for Text Extensions for Pandas\n",
    "### Part 1\n",
    "\n",
    "To run this notebook, you will need to obtain a copy of the CoNLL-2003 data set's corpus.\n",
    "Drop the corpus's files into the following locations:\n",
    "* conll_03/eng.testa\n",
    "* conll_03/eng.testb\n",
    "* conll_03/eng.train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZATION BOILERPLATE\n",
    "\n",
    "# The Jupyter kernel for this notebook usually starts up inside the notebooks\n",
    "# directory, but the text_extensions_for_pandas package code is in the parent\n",
    "# directory. Add that parent directory to the front of the Python include path.\n",
    "import sys\n",
    "if (sys.path[0] != \"..\"):\n",
    "    sys.path[0] = \"..\"\n",
    "    \n",
    "# Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# And of course we need the text_extensions_for_pandas library itself.\n",
    "import text_extensions_for_pandas as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read gold standard data for the validation set.\n",
    "\n",
    "# Note that this data is NOT kept in source control; you need to obtain \n",
    "# an appropriate license and download the data yourself separately to\n",
    "# run this notebook.\n",
    "\n",
    "# Note also that the original corpus started with the special \"-DOCSTART-\"\n",
    "# tag, while other versions start right in with the \n",
    "# If you have one of those other versions, you'll need to add the following\n",
    "# two lines at the beginning to make the tokens line up with the output in\n",
    "# ner.tgz:\n",
    "# -DOCSTART- O\n",
    "# \n",
    "# ^^^ note blank line after special token.\n",
    "#\n",
    "# If you need to add those lines, you should also remove the extra \n",
    "# \"-DOCSTART-\" token at the end of each file.\n",
    "\n",
    "gold_standard = tp.conll_2003_to_dataframes(\"../conll_03/eng.testa\")\n",
    "\n",
    "# tp.conll_2003_to_dataframes() returns a list of dataframes\n",
    "len(gold_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_standard[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the outputs of the \"bender\" team in the original competition.\n",
    "# Yes, this file is called \"testa\" in one data set and \"testb\" in the other.\n",
    "# Go figure.\n",
    "# Also, we needed to remove the first two lines of the output file, because \n",
    "# the original version corpus apparently started with \"-DOCSTART-\", while the\n",
    "# new one does not.\n",
    "bender_output = tp.conll_2003_output_to_dataframes(\n",
    "    gold_standard, \"../resources/conll_03/ner/results/bender/eng.testb\")\n",
    "bender_output[0].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the gold standard to spans.\n",
    "# Again, one dataframe per document.\n",
    "gold_standard_spans = [tp.iob_to_spans(df) for df in gold_standard]\n",
    "bender_output_spans = [tp.iob_to_spans(df) for df in bender_output]\n",
    "bender_output_spans[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at just PER annotations\n",
    "gold_person = [df[df[\"ent_type\"] == \"PER\"] for df in gold_standard_spans]\n",
    "bender_person = [df[df[\"ent_type\"] == \"PER\"] for df in bender_output_spans]\n",
    "gold_person[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also ask these span columns to render themselves to HTML for a\n",
    "# closer look at the target document.\n",
    "bender_person[0][\"token_span\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's show how to evaluate these results against the gold standard.\n",
    "# We could look at exact matches...\n",
    "gold_person[0].merge(bender_person[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...or we could give credit for partial matches contained entirely \n",
    "# within a true match:\n",
    "tp.contain_join(gold_person[0][\"token_span\"], bender_person[0][\"token_span\"], \"gold\", \"extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...or we could give credit for matches that overlap at all with\n",
    "# a true match:\n",
    "tp.overlap_join(gold_person[0][\"token_span\"], bender_person[0][\"token_span\"], \"gold\", \"extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's stick with exact matches for now.\n",
    "# Iterate over the pairs of dataframes for all the documents finding the\n",
    "# inputs we need to compute precision and recall for each document, and\n",
    "# wrap these values in a new dataframe.\n",
    "num_true_positives = [len(gold_person[i].merge(bender_person[i]).index)\n",
    "                      for i in range(len(gold_person))]\n",
    "num_extracted = [len(df.index) for df in bender_person]\n",
    "num_entities = [len(df.index) for df in gold_person]\n",
    "doc_num = np.arange(len(gold_person))\n",
    "\n",
    "stats_by_doc = pd.DataFrame({\n",
    "    \"doc_num\": doc_num,\n",
    "    \"num_true_positives\": num_true_positives,\n",
    "    \"num_extracted\": num_extracted,\n",
    "    \"num_entities\": num_entities\n",
    "})\n",
    "stats_by_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collection-wide precision and recall can be computed by aggregating\n",
    "# our dataframe:\n",
    "num_true_positives = stats_by_doc[\"num_true_positives\"].sum()\n",
    "num_entities = stats_by_doc[\"num_entities\"].sum()\n",
    "num_extracted = stats_by_doc[\"num_extracted\"].sum()\n",
    "\n",
    "precision = num_true_positives / num_extracted\n",
    "recall = num_true_positives / num_entities\n",
    "F1 = 2.0 * (precision * recall) / (precision + recall)\n",
    "print(\n",
    "\"\"\"Number of correct answers: {}\n",
    "Number of entities identified: {}\n",
    "Actual number of entities: {}\n",
    "Precision: {:1.2f}\n",
    "Recall: {:1.2f}\n",
    "F1: {:1.2f}\"\"\".format(num_true_positives, num_entities, num_entities, precision, recall, F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also add some additional columns with per-document stats:\n",
    "stats_by_doc[\"precision\"] = stats_by_doc[\"num_true_positives\"] / stats_by_doc[\"num_extracted\"]\n",
    "stats_by_doc[\"recall\"] = stats_by_doc[\"num_true_positives\"] / stats_by_doc[\"num_entities\"]\n",
    "stats_by_doc[\"F1\"] = 2.0 * (stats_by_doc[\"precision\"] * stats_by_doc[\"recall\"]) / (stats_by_doc[\"precision\"] + stats_by_doc[\"recall\"])\n",
    "stats_by_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's zero in on the ten most problematic documents by F1 score.\n",
    "stats_by_doc.sort_values(\"F1\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's happening with document 75?\n",
    "gold_person[75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bender_person[75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
