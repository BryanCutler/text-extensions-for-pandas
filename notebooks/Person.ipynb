{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Person.ipynb\n",
    "\n",
    "Demonstration notebook for Text Extensions for Pandas.\n",
    "\n",
    "This notebook evaluates the effectiveness of a person name extractor using the facilities of Text Extensions for Pandas.\n",
    "\n",
    "Instructions to run:\n",
    "1. (optional) Use the script `env.sh` at the root of this project to create an Anaconda environment `pd` with required packages. Activate this environment by typing `conda activate pd`.\n",
    "1. From a shell window at the root of the project, start up JupyterLab by typing `jupyter lab`\n",
    "1. Inside JupyterLab, navigate to the `notebooks` directory and open up this notebook. You should now be able to run the code in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZATION BOILERPLATE\n",
    "\n",
    "# The Jupyter kernel for this notebook usually starts up inside the notebooks\n",
    "# directory, but the text_extensions_for_pandas package code is in the parent\n",
    "# directory. Add that parent directory to the front of the Python include path.\n",
    "import sys\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.insert(0, \"..\")\n",
    "\n",
    "# Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex\n",
    "import spacy\n",
    "spacy_language_model = spacy.load(\"en_core_web_sm\")\n",
    "import textwrap\n",
    "\n",
    "# And of course we need the text_extensions_for_pandas library itself.\n",
    "import text_extensions_for_pandas as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example document text courtesy https://en.wikipedia.org/wiki/Monty_Python_and_the_Holy_Grail\n",
    "# License: CC-BY-SA\n",
    "with open (\"../resources/holy_grail.txt\", \"r\") as f:\n",
    "    doc_text = f.read()\n",
    "    pass\n",
    " \n",
    "# Parse the document text with SpaCy, then convert the results to a dataframe\n",
    "token_features = tp.make_tokens_and_features(doc_text, spacy_language_model)\n",
    "token_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can extract out all unique sentence spans by aggregating the \"sentence\" \n",
    "# column of the above dataframe:\n",
    "sentences = pd.DataFrame({\"sentence\": token_features[\"sentence\"].unique()})\n",
    "sentences.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"ent_iob\" and \"ent_type\" fields contain entity tags in \n",
    "# Inside-Outside-Beginning (IOB) format.\n",
    "# Text Extensions for Pandas has a built-in function to convert \n",
    "# IOB tagged data to spans of entities.\n",
    "entities = tp.iob_to_spans(token_features)\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at just the entities tagged \"PERSON\"\n",
    "person_entities = entities[entities[\"ent_type\"] == \"PERSON\"]\n",
    "person_entities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the TokenSpanArray's built-in HTML rendering to look at these\n",
    "# PERSON entities in the context of the document.\n",
    "person_entities[\"token_span\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gold standard labels in IOB format from a CSV file\n",
    "person_gold_iob = pd.read_csv(\"../resources/holy_grail_person.csv\")\n",
    "\n",
    "# Pull in token offsets from our token_features dataframe\n",
    "person_gold_iob[\"token_span\"] = token_features[\"token_span\"].values\n",
    "person_gold_iob[\"char_span\"] = token_features[\"char_span\"].values\n",
    "person_gold_iob.iloc[25:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from IOB format to spans of entities\n",
    "person_gold = tp.iob_to_spans(person_gold_iob, entity_type_col_name=None)\n",
    "person_gold.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the spans that are in both the extractor's answer set and the gold standard\n",
    "person_intersection = person_gold.merge(person_entities)\n",
    "person_intersection.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute precision and recall, just on this document.\n",
    "# Of course, in a real use case, we would be computing these values on a \n",
    "# development holdout set of documents while tuning the model, then\n",
    "# computing them again on a validation set during final testing.\n",
    "# We use a single document here to show that it is straightforward \n",
    "# to collect the necessary information using Pandas.\n",
    "num_true_positives = len(person_intersection.index)\n",
    "num_entities = len(person_gold.index)\n",
    "num_entities_extracted = len(person_entities.index)\n",
    "\n",
    "precision = num_true_positives / num_entities_extracted\n",
    "recall = num_true_positives / num_entities\n",
    "F1 = 2.0 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\n",
    "\"\"\"Number of correct answers: {}\n",
    "Number of entities identified: {}\n",
    "Actual number of entities: {}\n",
    "Precision: {:1.2f}\n",
    "Recall: {:1.2f}\n",
    "F1: {:1.2f}\"\"\".format(num_true_positives, num_entities, num_entities_extracted, precision, recall, F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That seems a bit low. Let's look at the false positives.\n",
    "false_positives = person_entities[~person_entities[\"token_span\"].isin(person_gold[\"token_span\"])]\n",
    "false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hmm, aside from the first three, most of these appear to be partial matches.\n",
    "# Let's recompute precision and recall giving credit for partial matches.\n",
    "# We start by finding out how many spans in person_entities[\"token_span\"]\n",
    "# are contained within a span from person_gold[\"token_span\"]\n",
    "looser_intersection = tp.contain_join(person_gold[\"token_span\"], person_entities[\"token_span\"],\n",
    "                                      \"gold\", \"extracted\")\n",
    "looser_intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that there are some duplicates (rows 23 and 24, for example).\n",
    "# Use the number of unique values in the \"gold\" column to compute\n",
    "# how many partial or complete matches of an entity we found.\n",
    "num_unique_matches = len(looser_intersection[\"gold\"].unique())\n",
    "num_unique_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompute precision, recall, and F1 score on this looser basis.\n",
    "# Again, in a real use case we would be doing this operation on a holdout set of\n",
    "# multiple documents. The point here is that the core computations all map\n",
    "# easily into Pandas.\n",
    "num_true_positives = num_unique_matches\n",
    "num_entities = len(person_gold.index)\n",
    "num_entities_extracted = len(person_entities.index)\n",
    "\n",
    "precision = num_true_positives / num_entities_extracted\n",
    "recall = num_true_positives / num_entities\n",
    "F1 = 2.0 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\n",
    "\"\"\"Number of correct answers: {}\n",
    "Number of entities identified: {}\n",
    "Actual number of entities: {}\n",
    "Precision: {:1.2f}\n",
    "Recall: {:1.2f}\n",
    "F1: {:1.2f}\"\"\".format(num_true_positives, num_entities, num_entities_extracted, precision, recall, F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's drill down on those partial matches to see what's causing them\n",
    "# (at least on this one document)\n",
    "partial_matches = looser_intersection[looser_intersection[\"gold\"].values != looser_intersection[\"extracted\"].values].reset_index(drop=True)\n",
    "partial_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hmm, there seems to be some clustering of the matches. Let's see how\n",
    "# they map onto the sentences of the document.\n",
    "extracted_sentence = tp.contain_join(sentences[\"sentence\"], partial_matches[\"extracted\"],\n",
    "                                     first_name=\"sentence\")\n",
    "partial_matches[\"sentence\"] = extracted_sentence[\"sentence\"].values\n",
    "partial_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like 1/3 of the partial matches on this document are clustered in a \n",
    "# single problem sentence. Let's take a closer look at that sentence.\n",
    "sentence_span = partial_matches[\"sentence\"].loc[0]\n",
    "sentence = token_features[token_features[\"sentence\"] == sentence_span]\n",
    "sentence.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SpaCy to render the dependency parse of the sentence\n",
    "tp.render_parse_tree(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That's a lot of parse tree! Let's cut that down to the tokens\n",
    "# that cover entities from the gold standard data.\n",
    "entity_tokens = tp.contain_join(person_gold[\"token_span\"], sentence[\"token_span\"],\n",
    "                                \"entity\", \"token_span\")\n",
    "entity_tokens.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract out and display the part of the dependency parse that covers just those tokens\n",
    "mask = token_features[\"token_span\"].isin(entity_tokens[\"token_span\"])\n",
    "partial_parse = token_features[mask]\n",
    "tp.render_parse_tree(partial_parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the filtered parse tree, two things pop out:\n",
    "# 1. The dependency parser model finds information about proper noun phrases\n",
    "#    that the NER model does not catch.\n",
    "# 2. The phrase \"Sir Not-Appearing-in-this-Film\" causes the dependency parser \n",
    "#    model to go off the rails.\n",
    "#\n",
    "# Let's see if we can combine the results of the two models to get more accurate\n",
    "# spans.\n",
    "# First, let's use some Gremlin to extract out the compound proper nouns from\n",
    "# the parse tree. We'll do this at the document level.\n",
    "g = tp.token_features_to_traversal(token_features)\n",
    "compound_proper_nouns = (\n",
    "    g.V()  # Start with all vertices.\n",
    "    .has(\"tag\", \"NNP\")  # Filter out those not tagged NNP (proper noun).\n",
    "    .has(\"dep\", \"compound\").as_(\"src\")  # Filter out those without a dependency link of type \"compound\".\n",
    "    .out()  # Follow the outgoing link to the parent node.\n",
    "    .has(\"tag\", \"NNP\").as_(\"dest\")  # Filter paths where the parent node is not a proper noun.\n",
    "    .select(\"src\", \"dest\").by(\"token_span\")  # Return parents of tokens\n",
    ").toDataFrame()\n",
    "# Add a third column with the combined span\n",
    "compound_proper_nouns[\"phrase\"] = compound_proper_nouns[\"src\"] + compound_proper_nouns[\"dest\"]\n",
    "compound_proper_nouns.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find the cases where a compound proper noun from the deep parser\n",
    "# overlaps (but does not exactly match) with a person entity from the \n",
    "# named entity recognizer.\n",
    "overlap = tp.overlap_join(compound_proper_nouns[\"phrase\"], person_entities[\"token_span\"],\n",
    "                          first_name=\"compound_phrase\", second_name=\"person\")\n",
    "strict_overlap = overlap[~overlap[\"compound_phrase\"].isin(person_entities[\"token_span\"])].reset_index(drop=True)\n",
    "strict_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use these pairs of spans to build up expanded person spans\n",
    "strict_overlap[\"expanded_person\"] = strict_overlap[\"compound_phrase\"] + strict_overlap[\"person\"]\n",
    "strict_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we just added these expanded spans back to our original set of \n",
    "# entities, we would get overlapping results. Find and filter out the \n",
    "# results from the original entities that overlap with our expanded\n",
    "# person entities.\n",
    "to_filter = tp.overlap_join(strict_overlap[\"expanded_person\"], person_entities[\"token_span\"],\n",
    "                            first_name=\"expanded_person\", second_name=\"token_span\")\n",
    "to_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the contents of to_filter and add the contents of strict_overlap to\n",
    "# our original set of persons\n",
    "filtered = person_entities[\"token_span\"][~person_entities[\"token_span\"].isin(to_filter[\"token_span\"])]\n",
    "person_entities_2 = pd.DataFrame({\"token_span\": \n",
    "                                  pd.concat([filtered, strict_overlap[\"expanded_person\"]])\n",
    "                                    .sort_values()\n",
    "                                    .reset_index(drop=True)})\n",
    "person_entities_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what this correction does to the exact-match precision and recall\n",
    "person_intersection_2 = person_gold.merge(person_entities_2)\n",
    "num_true_positives = len(person_intersection_2.index)\n",
    "num_entities = len(person_gold.index)\n",
    "num_entities_extracted = len(person_entities_2.index)\n",
    "\n",
    "precision = num_true_positives / num_entities_extracted\n",
    "recall = num_true_positives / num_entities\n",
    "F1 = 2.0 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\n",
    "\"\"\"Number of correct answers: {}\n",
    "Number of entities identified: {}\n",
    "Actual number of entities: {}\n",
    "Precision: {:1.2f}\n",
    "Recall: {:1.2f}\n",
    "F1: {:1.2f}\"\"\".format(num_true_positives, num_entities, num_entities_extracted, precision, recall, F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've just shown that you can quickly combine the results of multiple\n",
    "models using Pandas and Gremlin.\n",
    "\n",
    "It's important to note that the improvement in precision may or\n",
    "may not generalize to the other documents of the corpus. In a real use case, we would need to \n",
    "validate this approach against a development set of test documents. If this\n",
    "simple hybrid approach works well there, an appropriate next step would be \n",
    "to retrain the NER model using the dependency parser's \"compound\" tags as \n",
    "an additional feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now precision is looking pretty good, but recall is kind of low.\n",
    "# Let's examine the missing results.\n",
    "missing_results_mask = ~(person_gold[\"token_span\"].isin(looser_intersection[\"gold\"]))\n",
    "missing_results = person_gold[missing_results_mask]\n",
    "missing_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
