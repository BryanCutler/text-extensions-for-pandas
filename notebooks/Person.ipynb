{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Person.ipynb\n",
    "\n",
    "Demonstration notebook for Text Extensions for Pandas.\n",
    "\n",
    "This notebook creates business rules for a person name extractor using the facilities of Text Extensions for Pandas.\n",
    "\n",
    "Instructions to run:\n",
    "1. (optional) Use the script `env.sh` at the root of this project to create an Anaconda environment `pd` with required packages. Activate this environment by typing `conda activate pd`.\n",
    "1. From a shell window at the root of the project, start up JupyterLab by typing `jupyter lab`\n",
    "1. Inside JupyterLab, navigate to the `notebooks` directory and open up this notebook. You should now be able to run the code in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZATION BOILERPLATE\n",
    "\n",
    "# The Jupyter kernel for this notebook usually starts up inside the notebooks\n",
    "# directory, but the text_extensions_for_pandas package code is in the parent\n",
    "# directory. Add that parent directory to the front of the Python include path.\n",
    "import sys\n",
    "if (sys.path[0] != \"..\"):\n",
    "    sys.path[0] = \"..\"\n",
    "    \n",
    "# Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex\n",
    "import spacy\n",
    "spacy_language_model = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# And of course we need the text_extensions_for_pandas library itself.\n",
    "import text_extensions_for_pandas as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example document text courtesy https://en.wikipedia.org/wiki/Monty_Python_and_the_Holy_Grail\n",
    "# License: CC-BY-SA\n",
    "with open (\"../resources/holy_grail.txt\", \"r\") as f:\n",
    "    doc_text = f.read()\n",
    "    pass\n",
    " \n",
    "# Parse the document text with SpaCy, then convert the results to a dataframe\n",
    "token_features = tp.make_tokens_and_features(doc_text, spacy_language_model)\n",
    "token_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can extract out all unique sentence spans by aggregating the \"sentence\" \n",
    "# column of the above dataframe:\n",
    "sentences = pd.DataFrame({\"sentence\": token_features[\"sentence\"].unique()})\n",
    "sentences.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"ent_iob\" and \"ent_type\" fields contain entity tags in \n",
    "# Inside-Outside-Beginning (IOB) format.\n",
    "# Text Extensions for Pandas has a built-in function to convert \n",
    "# IOB tagged data to spans of entities.\n",
    "entities = tp.iob_to_spans(token_features)\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at just the entities tagged \"PERSON\"\n",
    "person_entities = entities[entities[\"ent_type\"] == \"PERSON\"]\n",
    "person_entities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the TokenSpanArray's built-in HTML rendering to look at these\n",
    "# PERSON entities in the context of the document.\n",
    "person_entities[\"token_span\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gold standard labels in IOB format from a CSV file\n",
    "person_gold_iob = pd.read_csv(\"../resources/holy_grail_person.csv\")\n",
    "\n",
    "# Pull in token offsets from our token_features dataframe\n",
    "person_gold_iob[\"token_span\"] = token_features[\"token_span\"].values\n",
    "person_gold_iob[\"char_span\"] = token_features[\"char_span\"].values\n",
    "person_gold_iob.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from IOB format to spans of entities\n",
    "person_gold = tp.iob_to_spans(person_gold_iob, entity_type_col_name=None)\n",
    "person_gold.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at one sentence in particular.\n",
    "sentence = token_features[token_features[\"sentence\"] == sentences[\"sentence\"][9]]\n",
    "sentence.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SpaCy to render the dependency parse of the sentence\n",
    "tp.render_parse_tree(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That's a lot of parse tree! Let's cut that down to the portions of the parse\n",
    "# that cover entities from the gold standard data.\n",
    "\n",
    "# TODO: Show partial parse trees of the entities in the gold standard and tagger output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Augment entity spans by following \"compound\" links in the dependency parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stuff below this paragraph needs to be reincorporated into the main flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a Gremlin query to find all compound proper nouns in the document\n",
    "g = tp.token_features_to_traversal(token_features)\n",
    "compound_nouns = (\n",
    "    g.V()  # Start with all vertices.\n",
    "    .has(\"tag\", \"NNP\")  # Filter out those not tagged NNP (proper noun).\n",
    "    .has(\"dep\", \"compound\").as_(\"src\")  # Filter out those without a dependency link of type \"compound\".\n",
    "    .out()  # Follow the outgoing link to the parent node.\n",
    "    .has(\"tag\", \"NNP\").as_(\"dest\")  # Filter paths where the parent node is not a proper noun.\n",
    "    .select(\"src\", \"dest\").by(\"token_span\")  # Return parents of tokens\n",
    ").toDataFrame()\n",
    "# Add a third column with the combined span\n",
    "compound_nouns[\"phrase\"] = tp.combine_spans(compound_nouns[\"src\"], compound_nouns[\"dest\"])\n",
    "compound_nouns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the locations of those compound nouns\n",
    "compound_nouns[\"phrase\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter down the example sentence to just the tokens that take part in compound nouns\n",
    "all_tokens_df = pd.DataFrame({\n",
    "    \"token_span\" : pd.concat([compound_nouns[c] for c in compound_nouns]).unique()})\n",
    "compound_noun_tokens = sentence.merge(all_tokens_df)\n",
    "compound_noun_tokens = compound_noun_tokens.set_index(compound_noun_tokens[\"id\"])\n",
    "compound_noun_tokens.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the partial parse trees of just those tokens\n",
    "tp.render_parse_tree(compound_noun_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resources:\n",
    "    \"\"\"\n",
    "    Data structures that are loaded once, as opposed to recreated on\n",
    "    every document. For convenience, we hang all of these data structures\n",
    "    off of a single Python object.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.LanguageModel = spacy.load(\"en_core_web_sm\")\n",
    "        self.Tokenizer = self.LanguageModel.Defaults.create_tokenizer(self.LanguageModel)\n",
    "        self.FirstNameDict = tp.load_dict(\"../resources/first_name.dict\", self.Tokenizer)\n",
    "        self.LastNameDict = tp.load_dict(\"../resources/last_name.dict\", self.Tokenizer)\n",
    "        self.CapsWordRegex = regex.compile(\"[A-Z][a-z]*\")\n",
    "\n",
    "        \n",
    "resources = Resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build some business rules that define some text features.\n",
    "# The rules are organized into Python classes.\n",
    "# The output of each rule is a Pandas DataFrame.\n",
    "\n",
    "# TEMPORARY until we can use Python 3.8 functools' built-in memoized property\n",
    "from memoized_property import memoized_property\n",
    "\n",
    "class Dictionaries:\n",
    "    \"\"\"\n",
    "    Rules that evaluate dictionaries against the document's raw tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self, d: Document, resources: Resources):\n",
    "        self._d = d\n",
    "        self._resources = resources\n",
    "    \n",
    "    @memoized_property\n",
    "    def FirstName(self):\n",
    "        return tp.extract_dict(self._d.Tokens, self._resources.FirstNameDict)\n",
    "    \n",
    "    @memoized_property\n",
    "    def LastName(self):\n",
    "        return tp.extract_dict(self._d.Tokens, self._resources.LastNameDict)\n",
    "\n",
    "class Regexes:\n",
    "    \"\"\"\n",
    "    Rules that evaluate regular expressions against the document's raw tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self, d: Document, resources: Resources):\n",
    "        self._d = d\n",
    "        self._resources = resources\n",
    "    \n",
    "    @property\n",
    "    def CapsWord(self):\n",
    "        \"\"\"\n",
    "        A single token that starts with a capital letter, with subsequent letters not\n",
    "        capitalized.\n",
    "        \"\"\"\n",
    "        return tp.extract_regex_tok(\n",
    "            tokens = self._d.Tokens,\n",
    "            compiled_regex = self._resources.CapsWordRegex)\n",
    "    \n",
    "\n",
    "class Morphology:\n",
    "    \"\"\"\n",
    "    Rules that filter tokens according to shallow linguistic features.\n",
    "    \"\"\"\n",
    "    def __init__(self, d: Document):\n",
    "        self._d = d\n",
    "        \n",
    "    @property\n",
    "    def ProperNounToken(self):\n",
    "        \"\"\"\n",
    "        Tokens that the part of speech tagger tagged as proper nouns.\n",
    "        \"\"\"\n",
    "        feats = self._d.TokenFeatures\n",
    "        return pd.DataFrame({\"match\": feats[\"token_span\"][feats[\"tag\"] == \"NNP\"]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the tokens labeled as proper nouns\n",
    "doc = Document(TEST_TEXT, resources)\n",
    "morph = Morphology(doc)\n",
    "morph.ProperNounToken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty-print the spans in ProperNounToken\n",
    "morph.ProperNounToken[\"match\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write some additional business rules that define a person extractor.\n",
    "# Note the use of a Python method to avoid duplicate code in the rules.\n",
    "    \n",
    "class PersonName:\n",
    "    \"\"\"\n",
    "    Rules that extract potential person name entities.\n",
    "    \"\"\"\n",
    "    def __init__(self, doc: Document, dicts: Dictionaries, regexes: Regexes,\n",
    "                 morphology: Morphology):\n",
    "        self._doc = doc\n",
    "        self._dicts = dicts\n",
    "        self._regexes = regexes\n",
    "        self._morphology = morphology\n",
    "\n",
    "    @staticmethod\n",
    "    def first_last_name(first: pd.DataFrame, last: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Generic <first name> <last name> pattern match. Subroutine of rules below.\n",
    "        \n",
    "        :param first: DataFrame of first names, with the name in the column \"match\".\n",
    "        \n",
    "        :param last: DataFrame of last names, with the name in the column \"match\".\n",
    "        \n",
    "        :returns: A DataFrame with all <first name> <last name> matches, including the\n",
    "            columns \"first_name\", \"last_name\", and \"name\" \n",
    "            (span that covers both first and last names)\n",
    "        \"\"\"\n",
    "        ret = tp.adjacent_join(\n",
    "            first_series = first[\"match\"],\n",
    "            second_series = last[\"match\"],\n",
    "            first_name = \"first_name\",\n",
    "            second_name = \"last_name\")\n",
    "        ret[\"name\"] = tp.combine_spans(ret[\"first_name\"], ret[\"last_name\"])\n",
    "        return ret\n",
    "    \n",
    "    @property\n",
    "    def Person1(self):\n",
    "        \"\"\"\n",
    "        <match of GlobalFirstName dict> <match of GlobalLastName dict>\n",
    "        \"\"\"\n",
    "        return PersonName.first_last_name(self._dicts.FirstName, self._dicts.LastName)\n",
    "    \n",
    "    @property\n",
    "    def Person2(self):\n",
    "        \"\"\"\n",
    "        <match of GlobalFirstName dict> <capitalized word>\n",
    "        \"\"\"\n",
    "        return PersonName.first_last_name(self._dicts.FirstName, self._regexes.CapsWord)\n",
    "    \n",
    "    @property\n",
    "    def Person3(self):\n",
    "        \"\"\"\n",
    "        <token labeled as proper noun> <match of GlobalLastName dict>\n",
    "        \"\"\"\n",
    "        return PersonName.first_last_name(self._morphology.ProperNounToken, self._dicts.LastName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our rules for a document\n",
    "doc = Document(TEST_TEXT, resources)\n",
    "dicts = Dictionaries(doc, resources)\n",
    "regexes = Regexes(doc, resources)\n",
    "morph = Morphology(doc)\n",
    "persons = PersonName(doc, dicts, regexes, morph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show one of the output DataFrames\n",
    "persons.Person3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a detailed view of the \"name\" column of the above DataFrame\n",
    "persons.Person3[\"name\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    \"\"\"\n",
    "    By convention, we \n",
    "    \"\"\"\n",
    "    def __init__(self, doc_text: str, resources: Resources):\n",
    "        self._text = doc_text\n",
    "        self._resources = resources\n",
    "        \n",
    "    @property\n",
    "    def Text(self):\n",
    "        return self._text\n",
    "    \n",
    "    @memoized_property\n",
    "    def TokenFeatures(self):\n",
    "        return tp.make_tokens_and_features(self._text, self._resources.LanguageModel)\n",
    "    \n",
    "    @memoized_property\n",
    "    def Sentence(self):\n",
    "        return pd.DataFrame({\"sentence\": self.TokenFeatures[\"sentence\"].unique()})\n",
    "    \n",
    "    @property\n",
    "    def Tokens(self):\n",
    "        \"\"\"\n",
    "        :return: tokens as a `pd.Series` backed by a `CharSpanArray`.\n",
    "        \"\"\"\n",
    "        return self.TokenFeatures[\"char_span\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
