{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Person.ipynb\n",
    "\n",
    "Demonstration notebook for Text Extensions for Pandas.\n",
    "\n",
    "This notebook creates business rules for a person name extractor using the facilities of Text Extensions for Pandas.\n",
    "\n",
    "Instructions to run:\n",
    "1. (optional) Use the script `env.sh` at the root of this project to create an Anaconda environment `pd` with required packages. Activate this environment by typing `conda activate pd`.\n",
    "1. From a shell window at the root of the project, start up JupyterLab by typing `jupyter lab`\n",
    "1. Inside JupyterLab, navigate to the `notebooks` directory and open up this notebook. You should now be able to run the code in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZATION BOILERPLATE\n",
    "\n",
    "# The Jupyter kernel for this notebook usually starts up inside the notebooks\n",
    "# directory, but the text_extensions_for_pandas package code is in the parent\n",
    "# directory. Add that parent directory to the front of the Python include path.\n",
    "import sys\n",
    "if (sys.path[0] != \"..\"):\n",
    "    sys.path[0] = \"..\"\n",
    "    \n",
    "# Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex\n",
    "import spacy\n",
    "spacy_language_model = spacy.load(\"en_core_web_sm\")\n",
    "import textwrap\n",
    "\n",
    "# And of course we need the text_extensions_for_pandas library itself.\n",
    "import text_extensions_for_pandas as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example document text courtesy https://en.wikipedia.org/wiki/Monty_Python_and_the_Holy_Grail\n",
    "# License: CC-BY-SA\n",
    "with open (\"../resources/holy_grail.txt\", \"r\") as f:\n",
    "    doc_text = f.read()\n",
    "    pass\n",
    " \n",
    "# Parse the document text with SpaCy, then convert the results to a dataframe\n",
    "token_features = tp.make_tokens_and_features(doc_text, spacy_language_model)\n",
    "token_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can extract out all unique sentence spans by aggregating the \"sentence\" \n",
    "# column of the above dataframe:\n",
    "sentences = pd.DataFrame({\"sentence\": token_features[\"sentence\"].unique()})\n",
    "sentences.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"ent_iob\" and \"ent_type\" fields contain entity tags in \n",
    "# Inside-Outside-Beginning (IOB) format.\n",
    "# Text Extensions for Pandas has a built-in function to convert \n",
    "# IOB tagged data to spans of entities.\n",
    "entities = tp.iob_to_spans(token_features)\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at just the entities tagged \"PERSON\"\n",
    "person_entities = entities[entities[\"ent_type\"] == \"PERSON\"]\n",
    "person_entities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the TokenSpanArray's built-in HTML rendering to look at these\n",
    "# PERSON entities in the context of the document.\n",
    "person_entities[\"token_span\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gold standard labels in IOB format from a CSV file\n",
    "person_gold_iob = pd.read_csv(\"../resources/holy_grail_person.csv\")\n",
    "\n",
    "# Pull in token offsets from our token_features dataframe\n",
    "person_gold_iob[\"token_span\"] = token_features[\"token_span\"].values\n",
    "person_gold_iob[\"char_span\"] = token_features[\"char_span\"].values\n",
    "person_gold_iob.iloc[25:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from IOB format to spans of entities\n",
    "person_gold = tp.iob_to_spans(person_gold_iob, entity_type_col_name=None)\n",
    "person_gold.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the spans that are in both the extractor's answer set and the gold standard\n",
    "person_intersection = person_gold.merge(person_entities)\n",
    "person_intersection.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision and recall\n",
    "num_true_positives = len(person_intersection.index)\n",
    "num_entities = len(person_gold.index)\n",
    "num_entities_extracted = len(person_entities.index)\n",
    "\n",
    "precision = num_true_positives / num_entities_extracted\n",
    "recall = num_true_positives / num_entities\n",
    "F1 = 2.0 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\n",
    "\"\"\"Number of correct answers: {}\n",
    "Number of entities identified: {}\n",
    "Actual number of entities: {}\n",
    "Precision: {:1.2f}\n",
    "Recall: {:1.2f}\n",
    "F1: {:1.2f}\"\"\".format(num_true_positives, num_entities, num_entities_extracted, precision, recall, F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Our baseline model produces an **F1 score of 0.52** on this document, \n",
    "which is not so good.\n",
    "\n",
    "We could at this point work to retrain the baseline model for this domain, \n",
    "but that approach would involve several difficulties. We would need to \n",
    "obtain and label additional documents to cover this and similar documents\n",
    "without introducing skew. And we would need to retrain a deep learning model, \n",
    "which is also a very nontrivial task.\n",
    "\n",
    "In the case of this model, those two steps are the *easy* part, because the\n",
    "model is trained on the OntoNotes corpus. Doing anything with that corpus\n",
    "for commercial purposes requires purchasing an expensive license from the\n",
    "Linguistic Data Consortium:\n",
    "\n",
    "![alt text](../resources/ontonotes_license.png)\n",
    "\n",
    "So instead, let's leave the model as-is for now and try some easier approaches\n",
    "to improve our accuracy for this domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The simplest form of domain adaptation is whitelists and blacklists.\n",
    "# Let's find some candidates for a blacklist by looking for spans that\n",
    "# the model frequently and incorrectly labels as PERSON entities.\n",
    "false_positives_mask = ~person_entities[\"token_span\"].isin(person_gold[\"token_span\"])\n",
    "false_positives = person_entities[false_positives_mask]\n",
    "false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Monty Python\" and \"Knights\" are highly unlikely to be PERSON entities.\n",
    "# Create a dictionary (gazetteer) to hold these and other blacklisted strings.\n",
    "!cat ../resources/person_blacklist.dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary as a dataframe\n",
    "blacklist_dict = tp.load_dict(\"../resources/person_blacklist.dict\", spacy_language_model)\n",
    "blacklist_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build up a dataframe of all spans that match the dictionary\n",
    "tokens = token_features[\"char_span\"]\n",
    "blacklist_matches = tp.extract_dict(tokens, blacklist_dict)\n",
    "blacklist_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude any extracted entities that overlap exactly with a blacklist match.\n",
    "mask = ~person_entities[\"token_span\"].isin(blacklist_matches[\"match\"].values)\n",
    "person_entities_2 = person_entities[mask]\n",
    "person_entities_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo F1 calculation\n",
    "def compute_and_print_accuracy(ents: pd.DataFrame):\n",
    "    person_intersection = person_gold.merge(ents)\n",
    "    num_true_positives = len(person_intersection.index)\n",
    "    num_entities = len(person_gold.index)\n",
    "    num_entities_extracted = len(ents.index)\n",
    "    precision = num_true_positives / num_entities_extracted\n",
    "    recall = num_true_positives / num_entities\n",
    "    F1 = 2.0 * (precision * recall) / (precision + recall)\n",
    "    print(textwrap.dedent(\"\"\"    Number of correct answers: {}\n",
    "    Number of entities identified: {}\n",
    "    Actual number of entities: {}\n",
    "    Precision: {:1.2f}\n",
    "    Recall: {:1.2f}\n",
    "    F1: {:1.2f}\"\"\".format(num_true_positives, num_entities, \n",
    "                          num_entities_extracted, precision, recall, F1)))\n",
    "    \n",
    "compute_and_print_accuracy(person_entities_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The blacklist improved our precision from 0.61 to 0.68. \n",
    "# Let's see what we can do to improve recall. \n",
    "# Here are the remaining false positives.\n",
    "false_positives_2 = person_entities_2[~person_entities_2[\"token_span\"].isin(person_gold[\"token_span\"])]\n",
    "false_positives_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most of these false positives appear to be partial matches of actual Person\n",
    "# entities.\n",
    "\n",
    "# TODO: Implement ContainsJoin and use it to identify cases where the SpaCy NER\n",
    "#  model's output span is part of an entity span from the gold standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = blacklist_matches[\"match\"].iloc[0]\n",
    "mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp2 = person_entities[\"token_span\"].iloc[0]\n",
    "mp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp == mp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Aggregate these partial matches by sentence to find the sentence (9)\n",
    "#  that has the most examples of these partial matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at that one \"problem\" sentence.\n",
    "sentence = token_features[token_features[\"sentence\"] == sentences[\"sentence\"][9]]\n",
    "sentence.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SpaCy to render the dependency parse of the sentence\n",
    "tp.render_parse_tree(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That's a lot of parse tree! Let's cut that down to the portions of the parse\n",
    "# that cover entities from the gold standard data.\n",
    "\n",
    "# TODO: Use ContainsJoin to filter down to the tokens that take part in the \n",
    "#  partial results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Augment entity spans by following \"compound\" links in the dependency parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stuff below this paragraph needs to be reincorporated into the main flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a Gremlin query to find all compound proper nouns in the document\n",
    "g = tp.token_features_to_traversal(token_features)\n",
    "compound_nouns = (\n",
    "    g.V()  # Start with all vertices.\n",
    "    .has(\"tag\", \"NNP\")  # Filter out those not tagged NNP (proper noun).\n",
    "    .has(\"dep\", \"compound\").as_(\"src\")  # Filter out those without a dependency link of type \"compound\".\n",
    "    .out()  # Follow the outgoing link to the parent node.\n",
    "    .has(\"tag\", \"NNP\").as_(\"dest\")  # Filter paths where the parent node is not a proper noun.\n",
    "    .select(\"src\", \"dest\").by(\"token_span\")  # Return parents of tokens\n",
    ").toDataFrame()\n",
    "# Add a third column with the combined span\n",
    "compound_nouns[\"phrase\"] = tp.combine_spans(compound_nouns[\"src\"], compound_nouns[\"dest\"])\n",
    "compound_nouns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the locations of those compound nouns\n",
    "compound_nouns[\"phrase\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter down the example sentence to just the tokens that take part in compound nouns\n",
    "all_tokens_df = pd.DataFrame({\n",
    "    \"token_span\" : pd.concat([compound_nouns[c] for c in compound_nouns]).unique()})\n",
    "compound_noun_tokens = sentence.merge(all_tokens_df)\n",
    "compound_noun_tokens = compound_noun_tokens.set_index(compound_noun_tokens[\"id\"])\n",
    "compound_noun_tokens.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the partial parse trees of just those tokens\n",
    "tp.render_parse_tree(compound_noun_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resources:\n",
    "    \"\"\"\n",
    "    Data structures that are loaded once, as opposed to recreated on\n",
    "    every document. For convenience, we hang all of these data structures\n",
    "    off of a single Python object.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.LanguageModel = spacy.load(\"en_core_web_sm\")\n",
    "        self.Tokenizer = self.LanguageModel.Defaults.create_tokenizer(self.LanguageModel)\n",
    "        self.FirstNameDict = tp.load_dict(\"../resources/first_name.dict\", self.Tokenizer)\n",
    "        self.LastNameDict = tp.load_dict(\"../resources/last_name.dict\", self.Tokenizer)\n",
    "        self.CapsWordRegex = regex.compile(\"[A-Z][a-z]*\")\n",
    "\n",
    "        \n",
    "resources = Resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build some business rules that define some text features.\n",
    "# The rules are organized into Python classes.\n",
    "# The output of each rule is a Pandas DataFrame.\n",
    "\n",
    "# TEMPORARY until we can use Python 3.8 functools' built-in memoized property\n",
    "from memoized_property import memoized_property\n",
    "\n",
    "class Dictionaries:\n",
    "    \"\"\"\n",
    "    Rules that evaluate dictionaries against the document's raw tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self, d: Document, resources: Resources):\n",
    "        self._d = d\n",
    "        self._resources = resources\n",
    "    \n",
    "    @memoized_property\n",
    "    def FirstName(self):\n",
    "        return tp.extract_dict(self._d.Tokens, self._resources.FirstNameDict)\n",
    "    \n",
    "    @memoized_property\n",
    "    def LastName(self):\n",
    "        return tp.extract_dict(self._d.Tokens, self._resources.LastNameDict)\n",
    "\n",
    "class Regexes:\n",
    "    \"\"\"\n",
    "    Rules that evaluate regular expressions against the document's raw tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self, d: Document, resources: Resources):\n",
    "        self._d = d\n",
    "        self._resources = resources\n",
    "    \n",
    "    @property\n",
    "    def CapsWord(self):\n",
    "        \"\"\"\n",
    "        A single token that starts with a capital letter, with subsequent letters not\n",
    "        capitalized.\n",
    "        \"\"\"\n",
    "        return tp.extract_regex_tok(\n",
    "            tokens = self._d.Tokens,\n",
    "            compiled_regex = self._resources.CapsWordRegex)\n",
    "    \n",
    "\n",
    "class Morphology:\n",
    "    \"\"\"\n",
    "    Rules that filter tokens according to shallow linguistic features.\n",
    "    \"\"\"\n",
    "    def __init__(self, d: Document):\n",
    "        self._d = d\n",
    "        \n",
    "    @property\n",
    "    def ProperNounToken(self):\n",
    "        \"\"\"\n",
    "        Tokens that the part of speech tagger tagged as proper nouns.\n",
    "        \"\"\"\n",
    "        feats = self._d.TokenFeatures\n",
    "        return pd.DataFrame({\"match\": feats[\"token_span\"][feats[\"tag\"] == \"NNP\"]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the tokens labeled as proper nouns\n",
    "doc = Document(TEST_TEXT, resources)\n",
    "morph = Morphology(doc)\n",
    "morph.ProperNounToken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty-print the spans in ProperNounToken\n",
    "morph.ProperNounToken[\"match\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write some additional business rules that define a person extractor.\n",
    "# Note the use of a Python method to avoid duplicate code in the rules.\n",
    "    \n",
    "class PersonName:\n",
    "    \"\"\"\n",
    "    Rules that extract potential person name entities.\n",
    "    \"\"\"\n",
    "    def __init__(self, doc: Document, dicts: Dictionaries, regexes: Regexes,\n",
    "                 morphology: Morphology):\n",
    "        self._doc = doc\n",
    "        self._dicts = dicts\n",
    "        self._regexes = regexes\n",
    "        self._morphology = morphology\n",
    "\n",
    "    @staticmethod\n",
    "    def first_last_name(first: pd.DataFrame, last: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Generic <first name> <last name> pattern match. Subroutine of rules below.\n",
    "        \n",
    "        :param first: DataFrame of first names, with the name in the column \"match\".\n",
    "        \n",
    "        :param last: DataFrame of last names, with the name in the column \"match\".\n",
    "        \n",
    "        :returns: A DataFrame with all <first name> <last name> matches, including the\n",
    "            columns \"first_name\", \"last_name\", and \"name\" \n",
    "            (span that covers both first and last names)\n",
    "        \"\"\"\n",
    "        ret = tp.adjacent_join(\n",
    "            first_series = first[\"match\"],\n",
    "            second_series = last[\"match\"],\n",
    "            first_name = \"first_name\",\n",
    "            second_name = \"last_name\")\n",
    "        ret[\"name\"] = tp.combine_spans(ret[\"first_name\"], ret[\"last_name\"])\n",
    "        return ret\n",
    "    \n",
    "    @property\n",
    "    def Person1(self):\n",
    "        \"\"\"\n",
    "        <match of GlobalFirstName dict> <match of GlobalLastName dict>\n",
    "        \"\"\"\n",
    "        return PersonName.first_last_name(self._dicts.FirstName, self._dicts.LastName)\n",
    "    \n",
    "    @property\n",
    "    def Person2(self):\n",
    "        \"\"\"\n",
    "        <match of GlobalFirstName dict> <capitalized word>\n",
    "        \"\"\"\n",
    "        return PersonName.first_last_name(self._dicts.FirstName, self._regexes.CapsWord)\n",
    "    \n",
    "    @property\n",
    "    def Person3(self):\n",
    "        \"\"\"\n",
    "        <token labeled as proper noun> <match of GlobalLastName dict>\n",
    "        \"\"\"\n",
    "        return PersonName.first_last_name(self._morphology.ProperNounToken, self._dicts.LastName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our rules for a document\n",
    "doc = Document(TEST_TEXT, resources)\n",
    "dicts = Dictionaries(doc, resources)\n",
    "regexes = Regexes(doc, resources)\n",
    "morph = Morphology(doc)\n",
    "persons = PersonName(doc, dicts, regexes, morph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show one of the output DataFrames\n",
    "persons.Person3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a detailed view of the \"name\" column of the above DataFrame\n",
    "persons.Person3[\"name\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    \"\"\"\n",
    "    By convention, we \n",
    "    \"\"\"\n",
    "    def __init__(self, doc_text: str, resources: Resources):\n",
    "        self._text = doc_text\n",
    "        self._resources = resources\n",
    "        \n",
    "    @property\n",
    "    def Text(self):\n",
    "        return self._text\n",
    "    \n",
    "    @memoized_property\n",
    "    def TokenFeatures(self):\n",
    "        return tp.make_tokens_and_features(self._text, self._resources.LanguageModel)\n",
    "    \n",
    "    @memoized_property\n",
    "    def Sentence(self):\n",
    "        return pd.DataFrame({\"sentence\": self.TokenFeatures[\"sentence\"].unique()})\n",
    "    \n",
    "    @property\n",
    "    def Tokens(self):\n",
    "        \"\"\"\n",
    "        :return: tokens as a `pd.Series` backed by a `CharSpanArray`.\n",
    "        \"\"\"\n",
    "        return self.TokenFeatures[\"char_span\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
