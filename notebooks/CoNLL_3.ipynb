{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoNLL-2003 Example for Text Extensions for Pandas\n",
    "### Part 3\n",
    "\n",
    "To run this notebook, you will need to obtain a copy of the CoNLL-2003 data set's corpus.\n",
    "Drop the corpus's files into the following locations:\n",
    "* conll_03/eng.testa\n",
    "* conll_03/eng.testb\n",
    "* conll_03/eng.train\n",
    "\n",
    "If you are unfamiliar with the basics of Text Extensions for Pandas, we recommend you \n",
    "start with Part 1 of this example.\n",
    "\n",
    "### Introduction\n",
    "\n",
    "At the end of part 2 of the demo, we've shown that there are incorrect labels hidden in the CoNLL-2003 validation set, and that you can pinpoint those incorrect labels by data-mining the results of the 16 models the competitors submitted.\n",
    "\n",
    "Our goal for the remainder of the demo is to pinpoint incorrect labels across the entire data set. The (rough) process to do so will be:\n",
    "\n",
    "1. Retokenize the entire corpus using a \"BERT-compatible\" tokenizer, and map the token/entity labels from the original corpus on to the new tokenization.\n",
    "2. Generate BERT embeddings for every token in the entire corpus in one pass, and store those embeddings in a dataframe column (of type TensorType) alongside the tokens and labels.\n",
    "3. Use the embeddings to quickly train multiple models at multiple levels of sophistication (something like: SVMs, random forests, and LSTMs with small and large numbers of hidden states). Split the corpus into 10 parts and perform a 10-fold cross-validation.\n",
    "4. Repeat the process from part 2 on each fold of the 10-fold cross-validation, comparing the outputs of every model on the validation set for each fold.\n",
    "5. Analyze the results of the models to pipoint potential incorrect labels. Inspect those labels manually and build up a list of labels that are actually incorrect.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZATION BOILERPLATE\n",
    "\n",
    "# The Jupyter kernel for this notebook usually starts up inside the notebooks\n",
    "# directory, but the text_extensions_for_pandas package code is in the parent\n",
    "# directory. Add that parent directory to the front of the Python include path.\n",
    "import sys\n",
    "if (sys.path[0] != \"..\"):\n",
    "    sys.path[0] = \"..\"\n",
    "    \n",
    "# Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "from typing import *\n",
    "\n",
    "import sklearn.pipeline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets\n",
    "from IPython.display import display\n",
    "\n",
    "# And of course we need the text_extensions_for_pandas library itself.\n",
    "import text_extensions_for_pandas as tp\n",
    "\n",
    "# Code to display a progress bar while iterating over a list of dataframes.\n",
    "def run_with_progress_bar(num_docs: int, fn):\n",
    "    _UPDATE_SEC = 0.1\n",
    "    result = [] # Type: List[pd.DataFrame]\n",
    "    last_update = time.time()\n",
    "    progress_bar = ipywidgets.IntProgress(0, 0, num_docs,\n",
    "                                          description=\"Starting...\",\n",
    "                                          layout=ipywidgets.Layout(width=\"100%\"),\n",
    "                                          style={\"description_width\": \"12%\"})\n",
    "    display(progress_bar)\n",
    "    for i in range(num_docs):\n",
    "        result.append(fn(i))\n",
    "        now = time.time()\n",
    "        if i == num_docs - 1 or now - last_update >= _UPDATE_SEC:\n",
    "            progress_bar.value = i + 1\n",
    "            progress_bar.description = f\"{i + 1}/{num_docs} docs\"\n",
    "            last_update = now\n",
    "    progress_bar.bar_style = \"success\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Retokenize with a BERT tokenizer.\n",
    "\n",
    "Retokenize the corpus using a \"BERT-compatible\" tokenizer, and map the token/entity labels from the original corpus on to the new tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the validation set in its original tokenization\n",
    "valid_raw = tp.conll_2003_to_dataframes(\"../conll_03/eng.testa\")\n",
    "\n",
    "# Pick out the dataframe for a single example document.\n",
    "example_df = valid_raw[5]\n",
    "example_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spans_df = tp.iob_to_spans(example_df)\n",
    "spans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retokenize the document's text with the BERT tokenizer\n",
    "#bert_model_name = \"bert-base-uncased\"\n",
    "#bert_model_name = \"bert-large-uncased\"\n",
    "bert_model_name = \"dslim/bert-base-NER\"\n",
    "\n",
    "tokenizer = transformers.BertTokenizerFast.from_pretrained(bert_model_name, \n",
    "                                                           add_special_tokens=True)\n",
    "bert_toks_df = tp.make_bert_tokens(example_df[\"char_span\"].values[0].target_text, tokenizer)\n",
    "bert_toks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT tokenization includes special zero-length tokens.\n",
    "bert_toks_df[bert_toks_df[\"special_tokens_mask\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align the BERT tokens with the original tokenization\n",
    "# Start by converting the elements of gold_standard_spans to character spans\n",
    "def align_spans(spans_df, toks_df):\n",
    "    original_spans = spans_df[\"token_span\"]\n",
    "    bert_token_spans = tp.TokenSpanArray.align_to_tokens(toks_df[\"char_span\"],\n",
    "                                                         original_spans)\n",
    "    return pd.DataFrame({\n",
    "        \"original_span\": original_spans,\n",
    "        \"bert_token_span\": bert_token_spans,\n",
    "        \"ent_type\": spans_df[\"ent_type\"]\n",
    "    })\n",
    "\n",
    "aligned_spans_df = align_spans(spans_df, bert_toks_df)\n",
    "aligned_spans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate IOB2 tags and entity labels that align with the BERT tokens.\n",
    "# See https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)\n",
    "def add_iob_tags(toks_df, aligned_spans_df) -> pd.DataFrame:\n",
    "    iob_df = toks_df.copy()\n",
    "    iob_df[\"ent_iob\"] = tp.spans_to_iob(aligned_spans_df[\"bert_token_span\"])\n",
    "    tmp_df = (\n",
    "        tp\n",
    "        .contain_join(aligned_spans_df[\"bert_token_span\"], iob_df[\"token_span\"],\n",
    "                      \"bert_token_span\", \"token_span\")\n",
    "        .merge(aligned_spans_df[[\"bert_token_span\", \"ent_type\"]])\n",
    "        [[\"token_span\", \"ent_type\"]]\n",
    "    )\n",
    "    iob_df = iob_df.merge(tmp_df, how=\"left\")\n",
    "    return iob_df\n",
    "\n",
    "iob_tags_df = add_iob_tags(bert_toks_df, aligned_spans_df)\n",
    "iob_tags_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The traditional way to transform NER to token classification is to \n",
    "# treat each combination of {I,O,B} X {entity type} as a different\n",
    "# class. Generate class labels in that format.\n",
    "# Create a Pandas categorical type for consistent encoding of categories\n",
    "# across all documents.\n",
    "_ENTITY_TYPES = [\"LOC\", \"MISC\", \"ORG\", \"PER\"]\n",
    "_ALL_LABELS = [\"O\"] + [f\"{x}-{y}\" for x in [\"B\", \"I\"] for y in _ENTITY_TYPES]\n",
    "token_class_dtype = pd.CategoricalDtype(categories=_ALL_LABELS)\n",
    "_LABEL_TO_INT = {_ALL_LABELS[i]: i for i in range(len(_ALL_LABELS))}\n",
    "\n",
    "\n",
    "def add_token_classes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    elems = []  # Type: str\n",
    "    for index, row in df[[\"ent_iob\", \"ent_type\"]].iterrows():\n",
    "        if row[\"ent_iob\"] == \"O\":\n",
    "            elems.append(\"O\")\n",
    "        else:\n",
    "            elems.append(f\"{row['ent_iob']}-{row['ent_type']}\")\n",
    "    ret = df.copy()\n",
    "    ret[\"token_class\"] = pd.Categorical(elems, dtype=token_class_dtype)\n",
    "    ret[\"token_class_id\"] = [_LABEL_TO_INT[l] for l in elems]\n",
    "    return ret\n",
    "\n",
    "classes_df = add_token_classes(iob_tags_df)\n",
    "classes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Add embeddings\n",
    "\n",
    "Generate BERT embeddings for every token in the entire corpus in one pass, \n",
    "and store those embeddings in a dataframe column (of type TensorType) \n",
    "alongside the tokens and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to attach some embeddings. Fire up a canned BERT model.\n",
    "bert = transformers.BertModel.from_pretrained(bert_model_name)\n",
    "\n",
    "# Level of indirection so we can more easily swap implementations\n",
    "def bert_fn(input_ids: np.ndarray, attention_mask: np.ndarray):\n",
    "    return bert(input_ids=torch.tensor(input_ids),\n",
    "                attention_mask=torch.tensor(attention_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that adds embeddings to a dataframe.\n",
    "def add_embeddings(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    _OVERLAP = 32\n",
    "    _NON_OVERLAP = 64\n",
    "    flat_input_ids = df[\"input_id\"].values\n",
    "    windows = tp.seq_to_windows(flat_input_ids, _OVERLAP, _NON_OVERLAP)\n",
    "    bert_result = bert_fn(input_ids=windows[\"input_ids\"], \n",
    "                          attention_mask=windows[\"attention_masks\"])\n",
    "    hidden_states = tp.windows_to_seq(flat_input_ids, \n",
    "                                      bert_result[0].detach().numpy(),\n",
    "                                      _OVERLAP, _NON_OVERLAP)\n",
    "    embeddings = tp.TensorArray(hidden_states)\n",
    "    ret = df.copy()\n",
    "    ret[\"embedding\"] = embeddings\n",
    "    return ret\n",
    "\n",
    "embeddings_df = add_embeddings(classes_df)\n",
    "embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the steps we've done so far into a single function.\n",
    "def process_doc(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    :param df: One dataframe from the conll_2003_to_dataframes() function,\n",
    "     representing the tokens of a single document in the original tokenization.\n",
    "    \n",
    "    :returns: A version of the same dataframe, but with BERT tokens, BERT\n",
    "     embeddings for each token, and token class labels.\n",
    "    \"\"\"\n",
    "    spans_df = tp.iob_to_spans(df)\n",
    "    bert_toks_df = tp.make_bert_tokens(df[\"char_span\"].values[0].target_text, \n",
    "                                       tokenizer)\n",
    "    aligned_spans_df = align_spans(spans_df, bert_toks_df)\n",
    "    iob_tags_df = add_iob_tags(bert_toks_df, aligned_spans_df)\n",
    "    classes_df = add_token_classes(iob_tags_df)\n",
    "    embeddings_df = add_embeddings(classes_df)\n",
    "    return embeddings_df\n",
    "\n",
    "# Rerun our example document to verify that our new function does the same \n",
    "# operations as the original code.\n",
    "process_doc(example_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training and test sets.\n",
    "train_raw = tp.conll_2003_to_dataframes(\"../conll_03/eng.train\")\n",
    "test_raw = tp.conll_2003_to_dataframes(\"../conll_03/eng.testb\")\n",
    "\n",
    "train_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the entire test set through our processing pipeline.\n",
    "test = run_with_progress_bar(\n",
    "    len(test_raw), lambda i: process_doc(test_raw[i]))\n",
    "test[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the entire validation set through our processing pipeline.\n",
    "valid = run_with_progress_bar(\n",
    "    len(valid_raw), lambda i: process_doc(valid_raw[i]))\n",
    "valid[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the entire training set through our processing pipeline.\n",
    "train = run_with_progress_bar(\n",
    "    len(train_raw), lambda i: process_doc(train_raw[i]))\n",
    "train[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single dataframe with the entire corpus's embeddings.\n",
    "def prep_for_stacking(collection_name: str, doc_num: int, df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return pd.DataFrame({\n",
    "        \"collection\": collection_name,\n",
    "        \"doc_num\": doc_num,\n",
    "        \"token_id\": df[\"id\"],\n",
    "        \"ent_iob\": df[\"ent_iob\"],\n",
    "        \"ent_type\": df[\"ent_type\"],\n",
    "        \"token_class\": df[\"token_class\"],\n",
    "        \"token_class_id\": df[\"token_class_id\"],\n",
    "        \"embedding\": df[\"embedding\"]\n",
    "    })\n",
    "\n",
    "to_stack = (\n",
    "    [prep_for_stacking(\"train\", i, train[i]) for i in range(len(train))]\n",
    "    + [prep_for_stacking(\"test\", i, test[i]) for i in range(len(test))]\n",
    "    + [prep_for_stacking(\"valid\", i, valid[i]) for i in range(len(valid))]\n",
    ")\n",
    "corpus_df = pd.concat(to_stack).reset_index(drop=True)\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the tokenized corpus with embeddings to a Feather file.\n",
    "corpus_df.to_feather(\"outputs/corpus.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stuff all the span information into a single dictionary of lists.\n",
    "# Dictionary index corresponds to the \"collection\" column of `corpus_df`,\n",
    "# and list index corresponds to the \"doc_num\" column of `curpos_df`.\n",
    "all_span_dfs = {\n",
    "    \"train\": train,\n",
    "    \"test\": test,\n",
    "    \"valid\": valid\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Train models\n",
    "\n",
    "Use the embeddings to quickly train multiple models at multiple levels of sophistication. Split the corpus into 10 parts and perform a 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional initialization boilerplate\n",
    "import sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus_df = pd.read_feather(\"outputs/corpus.feather\")\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = corpus_df[corpus_df[\"collection\"] == \"train\"]\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a multinomial logistic regression model on the training set.\n",
    "_MULTI_CLASS = \"multinomial\"\n",
    "base_pipeline = sklearn.pipeline.Pipeline([\n",
    "    # Standard scaler. This only makes a difference for certain classes\n",
    "    # of embeddings.\n",
    "    #(\"scaler\", sklearn.preprocessing.StandardScaler()),\n",
    "    (\"mlogreg\", sklearn.linear_model.LogisticRegression(\n",
    "        multi_class=_MULTI_CLASS,\n",
    "        verbose=10,\n",
    "        max_iter=10000\n",
    "    ))\n",
    "])\n",
    "\n",
    "X_train = train_df[\"embedding\"].values\n",
    "Y_train = train_df[\"token_class_id\"]\n",
    "base_model = base_pipeline.fit(X_train, Y_train)\n",
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the model's predict method in some pre/post processing\n",
    "def decode_tags(tags):\n",
    "    iobs = [\"O\" if t == \"O\" else t[:1] for t in tags]\n",
    "    types = [None if t == \"O\" else t.split(\"-\")[1] for t in tags]\n",
    "    return iobs, types\n",
    "\n",
    "def predict_on_df(df: pd.DataFrame, predictor):\n",
    "    id_to_class = df[\"token_class\"].values.categories.values\n",
    "\n",
    "    X = df[\"embedding\"].values\n",
    "    result_df = df.copy()\n",
    "    result_df[\"predicted_id\"] = predictor.predict(X)\n",
    "    result_df[\"predicted_class\"] = [id_to_class[i] for i in result_df[\"predicted_id\"].values]\n",
    "    iobs, types = decode_tags(result_df[\"predicted_class\"].values)\n",
    "    result_df[\"predicted_iob\"] = iobs\n",
    "    result_df[\"predicted_type\"] = types\n",
    "    return result_df\n",
    "\n",
    "# Look at results on the training set\n",
    "train_results_df = predict_on_df(train_df, base_model)\n",
    "train_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results_df.iloc[50:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at results on the validation set\n",
    "valid_results_df = predict_on_df(corpus_df[corpus_df[\"collection\"] == \"valid\"], base_model)\n",
    "valid_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_results_df.iloc[40:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split model outputs for an entire fold back into documents and add\n",
    "# token information.\n",
    "def split_by_doc(df: pd.DataFrame) -> List[pd.DataFrame]:\n",
    "    all_pairs = df[[\"collection\", \"doc_num\"]].drop_duplicates().to_records(index=False)\n",
    "    indexed_df = df.set_index([\"collection\", \"doc_num\", \"token_id\"], verify_integrity=True)\n",
    "    results = []\n",
    "    for collection, doc_num in all_pairs:\n",
    "        doc_slice = indexed_df.loc[collection, doc_num].reset_index()\n",
    "        doc_toks = all_span_dfs[collection][doc_num][\n",
    "            [\"id\", \"char_span\", \"token_span\", \"ent_iob\", \"ent_type\"]\n",
    "        ].rename(columns={\"id\": \"token_id\"})\n",
    "        result_df = doc_toks.copy().merge(\n",
    "            doc_slice[[\"token_id\", \"predicted_iob\", \"predicted_type\"]])\n",
    "        results.append(result_df)\n",
    "    return results\n",
    "\n",
    "valid_results_by_doc = split_by_doc(valid_results_df)\n",
    "valid_results_by_doc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert IOB-format output (and gold standard tags) to spans.\n",
    "def convert_to_spans(results_by_doc: pd.DataFrame):\n",
    "    actual_spans = [tp.iob_to_spans(v) for v in results_by_doc]\n",
    "    model_spans = [\n",
    "        tp.iob_to_spans(v, iob_col_name = \"predicted_iob\",\n",
    "                        entity_type_col_name = \"predicted_type\")\n",
    "          .rename(columns={\"predicted_type\": \"ent_type\"})\n",
    "        for v in results_by_doc]\n",
    "    return actual_spans, model_spans\n",
    "\n",
    "valid_actual_spans, valid_model_spans = convert_to_spans(valid_results_by_doc)\n",
    "valid_model_spans[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same per-document statistics calculation code as in CoNLL_2.ipynb\n",
    "def make_stats_df(gold_dfs, output_dfs):\n",
    "    num_true_positives = [len(gold_dfs[i].merge(output_dfs[i]).index)\n",
    "                          for i in range(len(gold_dfs))]\n",
    "    num_extracted = [len(df.index) for df in output_dfs]\n",
    "    num_entities = [len(df.index) for df in gold_dfs]\n",
    "    doc_num = np.arange(len(gold_dfs))\n",
    "\n",
    "    stats_by_doc = pd.DataFrame({\n",
    "        \"doc_num\": doc_num,\n",
    "        \"num_true_positives\": num_true_positives,\n",
    "        \"num_extracted\": num_extracted,\n",
    "        \"num_entities\": num_entities\n",
    "    })\n",
    "    stats_by_doc[\"precision\"] = stats_by_doc[\"num_true_positives\"] / stats_by_doc[\"num_extracted\"]\n",
    "    stats_by_doc[\"recall\"] = stats_by_doc[\"num_true_positives\"] / stats_by_doc[\"num_entities\"]\n",
    "    stats_by_doc[\"F1\"] = 2.0 * (stats_by_doc[\"precision\"] * stats_by_doc[\"recall\"]) / (stats_by_doc[\"precision\"] + stats_by_doc[\"recall\"])\n",
    "    return stats_by_doc\n",
    "\n",
    "valid_stats_by_doc = make_stats_df(valid_actual_spans, valid_model_spans)\n",
    "valid_stats_by_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collection-wide precision and recall can be computed by aggregating\n",
    "# our dataframe.\n",
    "def compute_global_scores(stats_by_doc: pd.DataFrame):\n",
    "    num_true_positives = stats_by_doc[\"num_true_positives\"].sum()\n",
    "    num_entities = stats_by_doc[\"num_entities\"].sum()\n",
    "    num_extracted = stats_by_doc[\"num_extracted\"].sum()\n",
    "\n",
    "    precision = num_true_positives / num_extracted\n",
    "    recall = num_true_positives / num_entities\n",
    "    F1 = 2.0 * (precision * recall) / (precision + recall)\n",
    "    return {\n",
    "        \"num_true_positives\": num_true_positives,\n",
    "        \"num_entities\": num_entities,\n",
    "        \"num_extracted\": num_extracted,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"F1\": F1\n",
    "    }\n",
    "\n",
    "compute_global_scores(valid_stats_by_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the above postprocessing steps into a single function.\n",
    "def analyze_model(target_df: pd.DataFrame, predictor):\n",
    "    \"\"\"\n",
    "    Score a model on a target set of documents.\n",
    "    \n",
    "    :param bert_df: Dataframe of tokens across documents with precomputed\n",
    "     embeddings in a column called \"embedding\"\n",
    "    :param predictor: Trained model with a `predict()` function that accepts\n",
    "     the contents of the \"embedding\" column of `bert_df`\n",
    "    \"\"\"\n",
    "    results_df = predict_on_df(target_df, predictor)\n",
    "    results_by_doc = split_by_doc(results_df)\n",
    "    actual_spans_by_doc, model_spans_by_doc = convert_to_spans(results_by_doc)\n",
    "    stats_by_doc = make_stats_df(actual_spans_by_doc, model_spans_by_doc)\n",
    "    return {\n",
    "        \"results_by_doc\": results_by_doc,\n",
    "        \"actual_spans_by_doc\": actual_spans_by_doc,\n",
    "        \"model_spans_by_doc\": model_spans_by_doc,\n",
    "        \"stats_by_doc\": stats_by_doc,\n",
    "        \"global_scores\": compute_global_scores(stats_by_doc)\n",
    "    }\n",
    "\n",
    "base_validation_results = analyze_model(corpus_df[corpus_df[\"collection\"] == \"valid\"],\n",
    "                                        base_model)\n",
    "base_validation_results[\"global_scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results on the training set\n",
    "base_train_results = analyze_model(corpus_df[corpus_df[\"collection\"] == \"train\"],\n",
    "                                   base_model)\n",
    "base_train_results[\"global_scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results on the test set\n",
    "base_test_results = analyze_model(corpus_df[corpus_df[\"collection\"] == \"test\"],\n",
    "                                  base_model)\n",
    "base_test_results[\"global_scores\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models with reduced result quality\n",
    "\n",
    "Define a function that produces detuned versions of our multilogreg model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.random_projection\n",
    "\n",
    "def train_reduced_model(X: np.ndarray, Y: np.ndarray, n_components: int, seed: 42):\n",
    "    \"\"\"\n",
    "    Train a reduced-quality model by putting a Gaussian random projection in\n",
    "    front of the multinomial logistic regression stage of the pipeline.\n",
    "    \n",
    "    :param X: input embeddings for training set\n",
    "    :param Y: integer labels corresponding to embeddings\n",
    "    :param n_components: Number of dimensions to reduce the embeddings to\n",
    "    :param seed: Random seed to drive Gaussian random projection\n",
    "    \"\"\"\n",
    "    reduce_pipeline = sklearn.pipeline.Pipeline([\n",
    "        (\"dimred\", sklearn.random_projection.GaussianRandomProjection(\n",
    "            n_components=n_components,\n",
    "            random_state=seed\n",
    "        )),\n",
    "        (\"mlogreg\", sklearn.linear_model.LogisticRegression(\n",
    "            multi_class=_MULTI_CLASS,\n",
    "            verbose=10,\n",
    "            max_iter=10000\n",
    "        ))\n",
    "    ])\n",
    "    print(f\"Training model with n_components={n_components} and seed={seed}.\")\n",
    "    return reduce_pipeline.fit(X, Y)\n",
    "\n",
    "reduce_model = train_reduced_model(X_train, Y_train, 16, None)\n",
    "reduce_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_validation_results = analyze_model(corpus_df[corpus_df[\"collection\"] == \"valid\"],\n",
    "                                          reduce_model)\n",
    "reduce_validation_results[\"global_scores\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Analyze model outputs\n",
    "\n",
    "Repeat the process from `CoNLL_2.ipynb` on each fold of the 10-fold cross-validation, comparing the outputs of every model on the validation set for each fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dry run: Train on the original training set\n",
    "\n",
    "Using the original CoNLL 2003 training set, train multiple models at\n",
    "different quality levels. Repeat the evaluation process \n",
    "from [`CoNLL_2.ipynb`](./CoNLL_2.ipynb) and verify that the ensemble\n",
    "of models can pinpoint incorrect labels in the validation data as in\n",
    "`CoNLL_2.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants for a grid over two parameters.\n",
    "# Not really a grid search, since every element of the grid provides value.\n",
    "_N_COMPONENTS = [8, 16, 32, 64, 128]  # Values for the n_components parameter\n",
    "_SEEDS = [1, 2, 3, 4]  # Values for the random seed\n",
    "\n",
    "params = [{\"n_components\": c, \"seed\": s} for c in _N_COMPONENTS for s in _SEEDS]\n",
    "\n",
    "def params_to_name(p):\n",
    "    return f\"{p['n_components']}_{p['seed']}\"\n",
    "\n",
    "models = {\n",
    "    params_to_name(p): train_reduced_model(X_train, Y_train,\n",
    "                                           p[\"n_components\"], p[\"seed\"])\n",
    "    for p in params\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df = corpus_df[corpus_df[\"collection\"] == \"valid\"]\n",
    "validation_results = {\n",
    "    name: analyze_model(validation_df, model) for name, model in models.items()\n",
    "}\n",
    "list(validation_results.values())[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_scores = [r[\"global_scores\"] for r in validation_results.values()]\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    \"n_components\": [p[\"n_components\"] for p in params],\n",
    "    \"seed\": [p[\"seed\"] for p in params],\n",
    "    \"name\": list(validation_results.keys()),\n",
    "    \"num_true_positives\": [r[\"num_true_positives\"] for r in global_scores],\n",
    "    \"num_entities\": [r[\"num_entities\"] for r in global_scores],\n",
    "    \"num_extracted\": [r[\"num_extracted\"] for r in global_scores],\n",
    "    \"precision\": [r[\"precision\"] for r in global_scores],\n",
    "    \"recall\": [r[\"recall\"] for r in global_scores],\n",
    "    \"F1\": [r[\"F1\"] for r in global_scores]\n",
    "})\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabulate all the results as in CoNLL_2.ipynb\n",
    "results = validation_results\n",
    "\n",
    "model_names = list(results.keys())\n",
    "first_model_name = model_names[0]\n",
    "first_results = results[first_model_name]\n",
    "gold_standard_by_doc = first_results[\"actual_spans_by_doc\"]\n",
    "num_docs = len(gold_standard_by_doc)\n",
    "\n",
    "\n",
    "def df_for_doc(i):\n",
    "    df = None\n",
    "    for model_name in model_names:\n",
    "        actual_spans_df = results[model_name][\"actual_spans_by_doc\"][i]\n",
    "        model_spans_df = results[model_name][\"model_spans_by_doc\"][i]\n",
    "        joined_results = pd.merge(actual_spans_df, model_spans_df, how=\"outer\", indicator=True)\n",
    "        joined_results[\"gold\"] = joined_results[\"_merge\"].isin([\"left_only\", \"both\"])\n",
    "        joined_results[model_name] = joined_results[\"_merge\"].isin([\"right_only\", \"both\"])\n",
    "        joined_results = joined_results.drop(columns=\"_merge\")\n",
    "        if df is None:\n",
    "            df = joined_results\n",
    "        else:\n",
    "            df = df.merge(joined_results, how=\"outer\", \n",
    "                          on=[\"token_span\", \"ent_type\", \"gold\"])           \n",
    "    # TokenSpanArrays from different documents can't currently be stacked,\n",
    "    # so convert to TokenSpan objects.\n",
    "    df[\"token_span\"] = df[\"token_span\"].astype(object)\n",
    "    df = df.fillna(False)\n",
    "    vectors = df[df.columns[3:]].values\n",
    "    counts = np.count_nonzero(vectors, axis=1)\n",
    "    df[\"num_models\"] = counts\n",
    "    df.insert(0, \"doc_num\", i)\n",
    "    return df\n",
    "\n",
    "to_stack = run_with_progress_bar(num_docs, df_for_doc)\n",
    "to_stack[42].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate all results as before\n",
    "all_results = pd.concat(\n",
    "    [df[[\"doc_num\", \"token_span\", \"ent_type\", \"gold\", \"num_models\"]] for df in to_stack]\n",
    ")\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many entities were found by zero models?\n",
    "(all_results[all_results[\"gold\"] == True][[\"num_models\", \"token_span\"]]\n",
    " .groupby(\"num_models\").count()\n",
    " .rename(columns={\"token_span\": \"count\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many non-results were found by many models?\n",
    "(all_results[all_results[\"gold\"] == False][[\"num_models\", \"token_span\"]]\n",
    " .groupby(\"num_models\").count()\n",
    " .rename(columns={\"token_span\": \"count\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardest results from the gold standard to get.\n",
    "# Use document ID to break ties.\n",
    "hard_to_get = all_results[all_results[\"gold\"]].sort_values(\"num_models\").head(20)\n",
    "hard_to_get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorrect results from the above 20:\n",
    "\n",
    "* Document 200: `[481, 486): 'Chris'`: Should be first word of `Chris Sutton` (`PER` entity)\n",
    "* Document 200: `[487, 493): 'Sutton'`: Should be first word of `Chris Sutton` (`PER` entity)\n",
    "* Document 220: `[834, 841): 'PACIFIC'`: Should be first word of `PACIFIC DIVISION` (`MISC` or `ORG` entity, not sure which)\n",
    "* Document 201: `[19, 35): 'NORTHERN IRELAND'`: Should be first two tokens of `NORTHERN IRELAND PREMIER DIVISION` (`MISC` or `ORG` entity, not sure which)\n",
    "* Document 56: `[84, 107): 'Department of Transport'`: Should be part of the larger `ORG` entity \"UK Department of Transport\"\n",
    "* Document 56: `[81, 83): 'UK'`: Should be part of the larger `ORG` entity \"UK Department of Transport\"\n",
    "* Document 55: `[129, 134): 'Czech'`: Should be labeled `MISC` because it's an adjective\n",
    "\n",
    "## Correct results:\n",
    "\n",
    "* Document 170, `[498, 509): 'Budisuryana'`: Name of a ship, difficult to identify as `MISC`\n",
    "* Document 60, `[930, 932): 'AA'`: Abbreviation for \"American Airlines\"\n",
    "* Document 220, `[981, 991): 'SACRAMENTO'`: Refers to the Sacramento basketball team\n",
    "* Document 220, `[952, 964): 'GOLDEN STATE'`: Basketball team\n",
    "* Document 220, `[807, 816): 'VANCOUVER'`: Basketball team\n",
    "* Document 60, `[547, 561): 'trans-Atlantic'`: Adjective form of \"Atlantic\", labeled as `MISC`\n",
    "* Document 60, `[981, 995): 'trans-Atlantic'`: Adjective form of \"Atlantic\", labeled as `MISC`\n",
    "* Document 60: `[346, 364): 'Trade and Industry'`: a British ministry\n",
    "* Document 57: `[65, 81): 'PT Tambang Timah'`: a company in Indonesia. \"PT\" stands for \"Perseroan Terbatas\", Indonesian for \"Limited Liability Company\".\n",
    "* Document 202: `[150, 156): 'Widnes'`: Refers to the Rugby team from Widnes\n",
    "* Document 56: `[11, 16): 'UK-US'`: Adjective referring to talks between two countries\n",
    "* Document 55: `[60, 66): 'PRAGUE'`: Location from dateline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratchpad for looking at individual docs\n",
    "# doc_results = gold_standard_by_doc[55]\n",
    "# doc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 of scratchpad\n",
    "# doc_results[\"token_span\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardest results from the gold standard to avoid\n",
    "hard_to_avoid = all_results[~all_results[\"gold\"]].sort_values(\"num_models\", ascending=False).head(20)\n",
    "hard_to_avoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing results in gold standard from the above 20:\n",
    "\n",
    "* Document 186: `[1280, 1296): 'Florence Masnada'`/`PER`) incorrectly labeled as `[1280, 1288): 'Florence'`/`LOC` and `[1289, 1296): 'Masnada'`/`PER`\n",
    "* Document 193: `[745, 760): 'United Province'`/`ORG` should be `LOC`, because it refers to a [the Indian province now known as Uttar Pradesh](https://en.wikipedia.org/wiki/United_Provinces_(1937%E2%80%9350))\n",
    "* Document 41: `[676, 690): 'Sporting Gijon'`/`ORG` incorrectly labeled as `[676, 684): 'Sporting'`/`ORG`\n",
    "* Document 222: `[93, 115): 'National Hockey League'`/`MISC` incorrectly labeled as `[93, 108): 'National Hockey'`/`ORG` and `[109, 115): 'League'`/`ORG` due to incorrect sentence boundary in corpus.\n",
    "* Document 31: `[561, 568): 'Schalke'`/`ORG` incorrectly labeled as `[561, 571): 'Schalke 04'`/`ORG`\n",
    "* Document 161: `[76, 98): 'John Lewis Partnership'`/`ORG` (NOTE: Could also be \"The John Lewis Partnership\") incorrectly labeled as `[76, 86): 'John Lewis'`/`PER`.\n",
    "* Document 213: `[697, 708): 'Dion Fourie'`/`PER` incorrectly labeled as `[697, 701): 'Dion'`/`PER` and `[702, 708): 'Fourie'`/`PER` due to incorrect sentence boundary in corpus.\n",
    "* Document 205: `[627, 636): 'Wimbledon'`/`ORG` (Wimbledon soccer team) incorrectly labeled as `LOC`\n",
    "* Document 199: `[487, 501): 'Robert Winters'`/`PER` incorrectly labeled as `[487, 493): 'Robert'` and `[494, 501): 'Winters'` due to incorrect sentence boundary in corpus.\n",
    "* Document 11: `[495, 509): 'Desvonde Botes'`/`PER` incorrectly labeled as `[495, 503): 'Desvonde'` and `[504, 509): 'Botes'` due to incorrect sentence boundary in corpus.\n",
    "* Document 223: `[289, 295): 'Ottawa'`/`ORG` incorrectly labeled as `LOC`.\n",
    "* Document 166: `[42, 50): 'NZ First'`/`ORG` (short for the New Zealand First political party) incorrectly labeled as `[42, 44): 'NZ'`/`LOC`\n",
    "* Document 168: `[443, 471): 'Australian Capital Territory'`/`LOC` (a [federal territory of Australia](https://en.wikipedia.org/wiki/Australian_Capital_Territory)) incorrectly labeled as `MISC`\n",
    "\n",
    "## Incorrect results in model output from above 20:\n",
    "\n",
    "* Document 180: `[286, 293): 'Malysia'`: From context, this is a misspelling of the country name \"Malaysia\".\n",
    "* Document 47: `[357, 364): 'English'`: First two syllables of \"Englishman\"\n",
    "* Document 31: `[532, 542): 'FC Cologne'`: Final two tokens of `[529, 542): '1. FC Cologne'`\n",
    "* Document 202: `[151, 156): 'idnes'`: Last 5 letters of `[150, 156): 'Widnes'`\n",
    "* Document 116: `[863, 869): 'Canola'`/`ORG`: Canola oil\n",
    "* Document 116: `[1123, 1134): 'Canola Corn'`/`ORG`: Two table headers for two types of vegetable oil\n",
    "* Document 180: `[876, 878): 'US'`/`MISC`: First token of \"US$\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratchpad for looking at individual docs\n",
    "# doc_num = 168\n",
    "# doc_results = gold_standard_by_doc[doc_num]\n",
    "# doc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 of scratchpad\n",
    "# doc_results[\"token_span\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3 of scratchpad (for looking at original tokenization)\n",
    "#valid_raw[doc_num].head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Inspect and correct\n",
    "\n",
    "Analyze the results of the models to pipoint potential incorrect labels. Inspect those labels manually and build up a list of labels that are actually incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
