{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoNLL-2003 Example for Text Extensions for Pandas\n",
    "### Part 3\n",
    "\n",
    "To run this notebook, you will need to obtain a copy of the CoNLL-2003 data set's corpus.\n",
    "Drop the corpus's files into the following locations:\n",
    "* conll_03/eng.testa\n",
    "* conll_03/eng.testb\n",
    "* conll_03/eng.train\n",
    "\n",
    "If you are unfamiliar with the basics of Text Extensions for Pandas, we recommend you \n",
    "start with Part 1 of this example.\n",
    "\n",
    "### Introduction\n",
    "\n",
    "At the end of part 2 of the demo, we've shown that there are incorrect labels hidden in the CoNLL-2003 validation set, and that you can pinpoint those incorrect labels by data-mining the results of the 16 models the competitors submitted.\n",
    "\n",
    "Our goal for part 3 of the demo is to pinpoint incorrect labels across the entire data set. The (rough) process to do so will be:\n",
    "\n",
    "1. Retokenize the entire corpus using a \"BERT-compatible\" tokenizer, and map the token/entity labels from the original corpus on to the new tokenization.\n",
    "2. Generate BERT embeddings for every token in the entire corpus in one pass, and store those embeddings in a dataframe column (of type TensorType) alongside the tokens and labels.\n",
    "3. Use the embeddings to quickly train multiple models at multiple levels of sophistication (something like: SVMs, random forests, and LSTMs with small and large numbers of hidden states). Split the corpus into 10 parts and perform a 10-fold cross-validation.\n",
    "4. Repeat the process from part 2 on each fold of the 10-fold cross-validation, comparing the outputs of every model on the validation set for each fold.\n",
    "5. ?\n",
    "6. Profit!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZATION BOILERPLATE\n",
    "\n",
    "# The Jupyter kernel for this notebook usually starts up inside the notebooks\n",
    "# directory, but the text_extensions_for_pandas package code is in the parent\n",
    "# directory. Add that parent directory to the front of the Python include path.\n",
    "import sys\n",
    "if (sys.path[0] != \"..\"):\n",
    "    sys.path[0] = \"..\"\n",
    "    \n",
    "# Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from typing import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets\n",
    "from IPython.display import display\n",
    "\n",
    "# And of course we need the text_extensions_for_pandas library itself.\n",
    "import text_extensions_for_pandas as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the corpus in its original tokenization\n",
    "gold_standard = tp.conll_2003_to_dataframes(\"../conll_03/eng.testa\")\n",
    "gold_standard_spans = [tp.iob_to_spans(df) for df in gold_standard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_standard[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_standard_spans[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retokenize the corpus's text with the BERT tokenizer\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "tokenizer = transformers.BertTokenizerFast.from_pretrained(bert_model_name, \n",
    "                                                           add_special_tokens=True)\n",
    "bert_toks = [tp.make_bert_tokens(df[\"char_span\"].values[0].target_text, tokenizer) for \n",
    "             df in gold_standard]\n",
    "bert_toks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT tokenization includes special zero-length tokens.\n",
    "doc_toks = bert_toks[0]\n",
    "doc_toks[doc_toks[\"special_tokens_mask\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align the BERT tokens with the original tokenization\n",
    "# Start by converting the elements of gold_standard_spans to character spans\n",
    "new_gold_standard_spans = []\n",
    "for i in range(len(gold_standard_spans)):\n",
    "    g = gold_standard_spans[i]\n",
    "    original_spans = g[\"token_span\"]\n",
    "    bert_token_spans = tp.TokenSpanArray.align_to_tokens(bert_toks[i][\"char_span\"],\n",
    "                                                         original_spans)\n",
    "    new_gold_standard_spans.append(pd.DataFrame({\n",
    "        \"original_span\": original_spans,\n",
    "        \"bert_token_span\": bert_token_spans,\n",
    "        \"ent_type\": g[\"ent_type\"]\n",
    "    }))                                            \n",
    "\n",
    "new_gold_standard_spans[1].head()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate IOB2 tags that align with the BERT tokens.\n",
    "# See https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)\n",
    "bert_gold_standard = [t.copy() for t in bert_toks]\n",
    "for i in range(len(bert_gold_standard)):\n",
    "    bert_gold_standard[i][\"ent_iob\"] = (\n",
    "        tp.spans_to_iob(new_gold_standard_spans[i][\"bert_token_span\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double-check that the new IOB2 tags are properly aligned\n",
    "bert_gold_standard[1].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the entity type tags to the new IOB2 data\n",
    "for i in range(len(bert_gold_standard)):\n",
    "    iob_df = bert_gold_standard[i]\n",
    "    entity_df = new_gold_standard_spans[i]\n",
    "    tmp_df = (\n",
    "        tp\n",
    "        .contain_join(entity_df[\"bert_token_span\"], iob_df[\"token_span\"],\n",
    "                      \"bert_token_span\", \"token_span\")\n",
    "        .merge(entity_df[[\"bert_token_span\", \"ent_type\"]])\n",
    "        [[\"token_span\", \"ent_type\"]]\n",
    "    )\n",
    "    bert_gold_standard[i] = iob_df.merge(tmp_df, how=\"left\")\n",
    "    \n",
    "# Print out the dataframe from the previous cell to show it has a new column\n",
    "bert_gold_standard[1].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to attach some embeddings. Fire up a canned BERT model.\n",
    "bert = transformers.BertModel.from_pretrained(bert_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for document 1, to show how it's done\n",
    "toks_df = bert_gold_standard[1]\n",
    "input_ids = torch.from_numpy(toks_df[\"input_id\"].values.reshape([1,-1]))\n",
    "bert_result = bert(input_ids=input_ids)\n",
    "hidden_states = bert_result[0]\n",
    "hidden_states.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for document 1, but this time run the document\n",
    "# through the BERT model in batches of 128 with the first and last \n",
    "# 32 in each batch overlapping with adjacent windows\n",
    "_OVERLAP = 32\n",
    "_NON_OVERLAP = 64\n",
    "\n",
    "toks_df = bert_gold_standard[1]\n",
    "flat_input_ids = toks_df[\"input_id\"].values\n",
    "windows = tp.seq_to_windows(flat_input_ids, _OVERLAP, _NON_OVERLAP)\n",
    "bert_result = bert(input_ids=torch.tensor(windows[\"input_ids\"]), \n",
    "                   attention_mask=torch.tensor(windows[\"attention_masks\"]))\n",
    "hidden_states = tp.windows_to_seq(flat_input_ids, \n",
    "                                  bert_result[0].detach().numpy(),\n",
    "                                  _OVERLAP, _NON_OVERLAP)\n",
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that adds embeddings to a dataframe.\n",
    "def add_embeddings(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    _OVERLAP = 32\n",
    "    _NON_OVERLAP = 64\n",
    "    flat_input_ids = df[\"input_id\"].values\n",
    "    windows = tp.seq_to_windows(flat_input_ids, _OVERLAP, _NON_OVERLAP)\n",
    "    bert_result = bert(input_ids=torch.tensor(windows[\"input_ids\"]), \n",
    "                       attention_mask=torch.tensor(windows[\"attention_masks\"]))\n",
    "    hidden_states = tp.windows_to_seq(flat_input_ids, \n",
    "                                      bert_result[0].detach().numpy(),\n",
    "                                      _OVERLAP, _NON_OVERLAP)\n",
    "    embeddings = tp.TensorArray(hidden_states)\n",
    "    ret = df.copy()\n",
    "    ret[\"embeddings\"] = embeddings\n",
    "    return ret\n",
    "\n",
    "# Test out the function on document 2.\n",
    "add_embeddings(bert_gold_standard[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can add embeddings to the entire gold standard data set.\n",
    "# This takes a while, so we display a progress bar.\n",
    "num_docs = len(bert_gold_standard)\n",
    "progress_bar = ipywidgets.IntProgress(0, 0, num_docs,\n",
    "                                      description=\"Starting...\",\n",
    "                                      layout=ipywidgets.Layout(width=\"100%\"),\n",
    "                                      style={\"description_width\": \"12%\"})\n",
    "display(progress_bar)\n",
    "for i in range(len(bert_gold_standard)):\n",
    "    bert_gold_standard[i] = add_embeddings(bert_gold_standard[i])\n",
    "    progress_bar.value = i + 1\n",
    "    progress_bar.description = f\"{i + 1}/{num_docs} docs\"\n",
    "progress_bar.bar_style = \"success\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the dataframe for document 5 look like now?\n",
    "bert_gold_standard[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
