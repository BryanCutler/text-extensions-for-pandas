{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoNLL-2003 Example for Text Extensions for Pandas\n",
    "### Part 3\n",
    "\n",
    "To run this notebook, you will need to obtain a copy of the CoNLL-2003 data set's corpus.\n",
    "Drop the corpus's files into the following locations:\n",
    "* conll_03/eng.testa\n",
    "* conll_03/eng.testb\n",
    "* conll_03/eng.train\n",
    "\n",
    "If you are unfamiliar with the basics of Text Extensions for Pandas, we recommend you \n",
    "start with Part 1 of this example.\n",
    "\n",
    "### Introduction\n",
    "\n",
    "At the end of part 2 of the demo, we've shown that there are incorrect labels hidden in the CoNLL-2003 validation set, and that you can pinpoint those incorrect labels by data-mining the results of the 16 models the competitors submitted.\n",
    "\n",
    "Our goal for part 3 of the demo is to pinpoint incorrect labels across the entire data set. The (rough) process to do so will be:\n",
    "\n",
    "1. Retokenize the entire corpus using a \"BERT-compatible\" tokenizer, and map the token/entity labels from the original corpus on to the new tokenization.\n",
    "2. Generate BERT embeddings for every token in the entire corpus in one pass, and store those embeddings in a dataframe column (of type TensorType) alongside the tokens and labels.\n",
    "3. Use the embeddings to quickly train multiple models at multiple levels of sophistication (something like: SVMs, random forests, and LSTMs with small and large numbers of hidden states). Split the corpus into 10 parts and perform a 10-fold cross-validation.\n",
    "4. Repeat the process from part 2 on each fold of the 10-fold cross-validation, comparing the outputs of every model on the validation set for each fold.\n",
    "5. ?\n",
    "6. Profit!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZATION BOILERPLATE\n",
    "\n",
    "# The Jupyter kernel for this notebook usually starts up inside the notebooks\n",
    "# directory, but the text_extensions_for_pandas package code is in the parent\n",
    "# directory. Add that parent directory to the front of the Python include path.\n",
    "import sys\n",
    "if (sys.path[0] != \"..\"):\n",
    "    sys.path[0] = \"..\"\n",
    "    \n",
    "# Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from typing import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets\n",
    "from IPython.display import display\n",
    "\n",
    "# And of course we need the text_extensions_for_pandas library itself.\n",
    "import text_extensions_for_pandas as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the corpus in its original tokenization\n",
    "gold_standard = tp.conll_2003_to_dataframes(\"../conll_03/eng.testa\")\n",
    "gold_standard_spans = [tp.iob_to_spans(df) for df in gold_standard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_standard[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_standard_spans[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retokenize the corpus's text with the BERT tokenizer\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "tokenizer = transformers.BertTokenizerFast.from_pretrained(bert_model_name, \n",
    "                                                           add_special_tokens=True)\n",
    "bert_toks = [tp.make_bert_tokens(df[\"char_span\"].values[0].target_text, tokenizer) for \n",
    "             df in gold_standard]\n",
    "bert_toks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT tokenization includes special zero-length tokens.\n",
    "doc_toks = bert_toks[0]\n",
    "doc_toks[doc_toks[\"special_tokens_mask\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align the BERT tokens with the original tokenization\n",
    "# Start by converting the elements of gold_standard_spans to character spans\n",
    "new_gold_standard_spans = []\n",
    "for i in range(len(gold_standard_spans)):\n",
    "    g = gold_standard_spans[i]\n",
    "    original_spans = g[\"token_span\"]\n",
    "    bert_token_spans = tp.TokenSpanArray.align_to_tokens(bert_toks[i][\"char_span\"],\n",
    "                                                         original_spans)\n",
    "    new_gold_standard_spans.append(pd.DataFrame({\n",
    "        \"original_span\": original_spans,\n",
    "        \"bert_token_span\": bert_token_spans,\n",
    "        \"ent_type\": g[\"ent_type\"]\n",
    "    }))                                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_gold_standard_spans[1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate IOB2 tags that align with the BERT tokens.\n",
    "# See https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)\n",
    "bert_gold_standard = [t.copy() for t in bert_toks]\n",
    "for i in range(len(bert_gold_standard)):\n",
    "    bert_gold_standard[i][\"ent_iob\"] = (\n",
    "        tp.spans_to_iob(new_gold_standard_spans[i][\"bert_token_span\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double-check that the new IOB2 tags are properly aligned\n",
    "bert_gold_standard[1].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the entity type tags to the new IOB2 data\n",
    "for i in range(len(bert_gold_standard)):\n",
    "    iob_df = bert_gold_standard[i]\n",
    "    entity_df = new_gold_standard_spans[i]\n",
    "    tmp_df = (\n",
    "        tp\n",
    "        .contain_join(entity_df[\"bert_token_span\"], iob_df[\"token_span\"],\n",
    "                      \"bert_token_span\", \"token_span\")\n",
    "        .merge(entity_df[[\"bert_token_span\", \"ent_type\"]])\n",
    "        [[\"token_span\", \"ent_type\"]]\n",
    "    )\n",
    "    bert_gold_standard[i] = iob_df.merge(tmp_df, how=\"left\")\n",
    "    \n",
    "# Print out the dataframe from the previous cell to show it has a new column\n",
    "bert_gold_standard[1].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to attach some embeddings. Fire up a canned BERT model.\n",
    "bert = transformers.BertModel.from_pretrained(bert_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for document 1, to show how it's done\n",
    "toks_df = bert_gold_standard[1]\n",
    "input_ids = torch.from_numpy(toks_df[\"input_id\"].values.reshape([1,-1]))\n",
    "bert_result = bert(input_ids=input_ids)\n",
    "hidden_states = bert_result[0]\n",
    "hidden_states.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to split documents into overlapping windows for computing embeddings\n",
    "# and to merge the results back.\n",
    "def seq_to_windows(seq: np.ndarray, overlap: int, non_overlap: int) \\\n",
    "        -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Convert a variable-length sequence into a set of fixed length windows,\n",
    "    adding padding as necessary.\n",
    "    \n",
    "    :param seq: Original variable length sequence, as a 1D numpy array\n",
    "    :param overlap: How much overlap there should be between adjacent windows\n",
    "    :param non_overlap: How much non-overlapping content between the overlapping\n",
    "     regions there should be at the middle of each window?\n",
    "     \n",
    "    :returns: A dictionary with two entries:\n",
    "      * `input_ids`: 2D `np.ndarray` of fixed-length windows\n",
    "      * `attention_masks`: 2D `np.ndarray` of attention masks (1 for\n",
    "        tokens that are NOT masked, 0 for tokens that are masked)\n",
    "        to feed into your favorite BERT-like embedding generator.\n",
    "    \"\"\"\n",
    "    if len(seq.shape) != 1:\n",
    "        raise ValueError(f\"Input array must be 1D; got shape {seq.shape}\")\n",
    "    window_length, pre_padding, post_padding = _compute_padding(\n",
    "        len(seq), overlap, non_overlap)\n",
    "    \n",
    "    # First generate the windows as a padded flat arrays.\n",
    "    padded_length = len(seq) + pre_padding + post_padding\n",
    "    buf = np.zeros(shape=[padded_length], dtype=seq.dtype)\n",
    "    buf[pre_padding:pre_padding + len(seq)] = seq\n",
    "    \n",
    "    mask_buf = np.zeros_like(buf, dtype=int)  # 0 == MASKED\n",
    "    mask_buf[pre_padding:pre_padding + len(seq)] = 1  # 1 == NOT MASKED\n",
    "\n",
    "    # Reshape the flat arrays into overlapping windows.\n",
    "    num_windows = padded_length // (overlap + non_overlap)\n",
    "    windows = np.zeros(shape=[num_windows, window_length], dtype=seq.dtype)\n",
    "    masks = np.zeros(shape=[num_windows, window_length], dtype=int)\n",
    "    for i in range(num_windows):\n",
    "        start = i * (overlap + non_overlap)\n",
    "        windows[i] = buf[start:start+window_length]\n",
    "        masks[i] = mask_buf[start:start+window_length]\n",
    "    return {\n",
    "        \"input_ids\": windows,\n",
    "        \"attention_masks\": masks\n",
    "    }\n",
    "\n",
    "def windows_to_seq(seq: np.ndarray, windows: np.ndarray,\n",
    "                   overlap: int, non_overlap: int) \\\n",
    "        -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Inverse of `seq_to_windows()`\n",
    "    Convert fixed length windows with padding to a variable-length sequence\n",
    "    that matches up with the original sequence from which the windows were\n",
    "    computed.\n",
    "    \n",
    "    :param seq: Original variable length sequence to align with,\n",
    "      as a 1D numpy array\n",
    "    :param windows: Windowed data to align with the original sequence.\n",
    "      Usually this data is the result of applying a transformation to the\n",
    "      output of `seq_to_windows()`\n",
    "    :param overlap: How much overlap there is between adjacent windows\n",
    "    :param non_overlap: How much non-overlapping content between the overlapping\n",
    "     regions there should be at the middle of each window?\n",
    "     \n",
    "    :returns: A 1D `np.ndarray` containing the contents of `windows` that \n",
    "     correspond to the elements of `seq`.\n",
    "    \"\"\"\n",
    "    if len(seq.shape) != 1:\n",
    "        raise ValueError(f\"Input array must be 1D; got shape {seq.shape}\")\n",
    "    window_length, pre_padding, post_padding = _compute_padding(\n",
    "        len(seq), overlap, non_overlap)\n",
    "    \n",
    "    # Input may be an array of n-dimensional tensors.\n",
    "    result_shape = [len(seq)] + list(windows.shape[2:])\n",
    "    #result = np.zeros_like(seq, dtype=windows.dtype)\n",
    "    result = np.zeros(shape=result_shape, dtype=windows.dtype)\n",
    "    \n",
    "    # Special-case the first and last windows.\n",
    "    if len(seq) <= non_overlap + (overlap//2):\n",
    "        # Only one window, potentially a partial window.\n",
    "        return windows[0][overlap:overlap+len(seq)]\n",
    "    else:\n",
    "        result[0:non_overlap+(overlap//2)] = \\\n",
    "            windows[0][overlap:overlap+non_overlap+(overlap//2)]\n",
    "    \n",
    "    num_to_copy_from_last = overlap//2 + overlap + non_overlap - post_padding\n",
    "    if num_to_copy_from_last > 0:\n",
    "        result[-num_to_copy_from_last:] = \\\n",
    "            windows[-1][overlap//2:(overlap//2)+num_to_copy_from_last]\n",
    "        \n",
    "    # Remaining windows can be covered in a loop\n",
    "    for i in range(1, len(windows) - 1):\n",
    "        src_start = overlap//2\n",
    "        dest_start = (overlap//2 + non_overlap\n",
    "                      + (i - 1) * (overlap + non_overlap))\n",
    "        num_to_copy = min(non_overlap + overlap, len(seq) - dest_start)\n",
    "        result[dest_start:dest_start+num_to_copy] = \\\n",
    "            windows[i][src_start:src_start+num_to_copy]\n",
    "            \n",
    "    return result\n",
    "        \n",
    "def _compute_padding(seq_len, overlap, non_overlap) \\\n",
    "        -> Tuple[int, int, int, int]:\n",
    "    \"\"\"\n",
    "    Shared padding computation for seq_to_windows() and windows_to_seq()\n",
    "    \n",
    "    :param seq_len: Length of original sequence\n",
    "    :param overlap: How much overlap there should be between adjacent window\n",
    "    :param non_overlap: How much non-overlapping content between the overlapping\n",
    "     regions there should be at the middle of each window?\n",
    "     \n",
    "    :returns: A tuple of (window_length, pre_padding, post_padding)\n",
    "    \"\"\"\n",
    "    if 0 != overlap % 2:\n",
    "        raise ValueError(f\"Non-even overlaps not implemented; got {overlap}\")\n",
    "        \n",
    "    # Each window has overlapping regions at the beginning and end\n",
    "    window_length = (2 * overlap) + non_overlap\n",
    "\n",
    "    # Account for the padding before the first window\n",
    "    pre_padding = overlap\n",
    "    \n",
    "    # Account for the padding after the last window\n",
    "    remainder = (seq_len + pre_padding) % (overlap + non_overlap)\n",
    "    post_padding = window_length - remainder\n",
    "    if post_padding == window_length:\n",
    "        # Chop off empty last window\n",
    "        post_padding -= overlap + non_overlap\n",
    "    \n",
    "    return (window_length, pre_padding, post_padding)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the functions we just wrote.\n",
    "for seqlen in range(1, 20):\n",
    "    seq = np.arange(1, seqlen)\n",
    "    seq_after = windows_to_seq(seq, seq_to_windows(seq, 2, 3)[\"input_ids\"], 2, 3)\n",
    "    if np.any(seq != seq_after):\n",
    "        raise ValueError(\"Before: {seq}; After: {seq_after}\")\n",
    "        \n",
    "for seqlen in range(200, 400):\n",
    "    # print(f\"seqlen: {seqlen}\")\n",
    "    seq = np.arange(1, seqlen)\n",
    "    windows = seq_to_windows(seq, 32, 64)\n",
    "    # print(f\"input_ids: {windows['input_ids']}\")\n",
    "    seq_after = windows_to_seq(seq, windows[\"input_ids\"], 32, 64)\n",
    "    if np.any(seq != seq_after):\n",
    "        raise ValueError(\"Before: {seq}; After: {seq_after}\")\n",
    "        \n",
    "for seqlen in range(50, 100):\n",
    "    # print(f\"seqlen: {seqlen}\")\n",
    "    seq = np.arange(1, seqlen)\n",
    "    windows = seq_to_windows(seq, 32, 64)\n",
    "    # print(f\"input_ids: {windows['input_ids']}\")\n",
    "    seq_after = windows_to_seq(seq, windows[\"input_ids\"], 32, 64)\n",
    "    if np.any(seq != seq_after):\n",
    "        raise ValueError(\"Before: {seq}; After: {seq_after}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run document 1 in batches of 128 with the first and last 32 in each\n",
    "# batch overlapping with adjacent windows\n",
    "_OVERLAP = 32\n",
    "_NON_OVERLAP = 64\n",
    "\n",
    "toks_df = bert_gold_standard[1]\n",
    "flat_input_ids = toks_df[\"input_id\"].values\n",
    "windows = seq_to_windows(flat_input_ids, _OVERLAP, _NON_OVERLAP)\n",
    "bert_result = bert(input_ids=torch.tensor(windows[\"input_ids\"]), \n",
    "                   attention_mask=torch.tensor(windows[\"attention_masks\"]))\n",
    "hidden_states = windows_to_seq(flat_input_ids, \n",
    "                               bert_result[0].detach().numpy(),\n",
    "                               _OVERLAP, _NON_OVERLAP)\n",
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that adds embeddings to a dataframe.\n",
    "def add_embeddings(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    _OVERLAP = 32\n",
    "    _NON_OVERLAP = 64\n",
    "    flat_input_ids = df[\"input_id\"].values\n",
    "    windows = seq_to_windows(flat_input_ids, _OVERLAP, _NON_OVERLAP)\n",
    "    bert_result = bert(input_ids=torch.tensor(windows[\"input_ids\"]), \n",
    "                       attention_mask=torch.tensor(windows[\"attention_masks\"]))\n",
    "    hidden_states = windows_to_seq(flat_input_ids, \n",
    "                                   bert_result[0].detach().numpy(),\n",
    "                                   _OVERLAP, _NON_OVERLAP)\n",
    "    embeddings = tp.TensorArray(hidden_states)\n",
    "    ret = df.copy()\n",
    "    ret[\"embeddings\"] = embeddings\n",
    "    return ret\n",
    "    \n",
    "add_embeddings(bert_gold_standard[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can add embeddings to the entire gold standard data set.\n",
    "# This takes a while, so we display a progress bar.\n",
    "num_docs = len(bert_gold_standard)\n",
    "progress_bar = ipywidgets.IntProgress(0, 0, num_docs,\n",
    "                                      description=\"Running...\",\n",
    "                                      layout=ipywidgets.Layout(width='100%'))\n",
    "display(progress_bar)\n",
    "for i in range(len(bert_gold_standard)):\n",
    "    bert_gold_standard[i] = add_embeddings(bert_gold_standard[i])\n",
    "    progress_bar.value = i + 1\n",
    "    progress_bar.description = f\"{i + 1}/{num_docs} docs\"\n",
    "progress_bar.bar_style = \"success\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the dataframe for document 5 look like now?\n",
    "bert_gold_standard[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
