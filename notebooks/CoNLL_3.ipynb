{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoNLL-2003 Example for Text Extensions for Pandas\n",
    "### Part 3\n",
    "\n",
    "To run this notebook, you will need to obtain a copy of the CoNLL-2003 data set's corpus.\n",
    "Drop the corpus's files into the following locations:\n",
    "* conll_03/eng.testa\n",
    "* conll_03/eng.testb\n",
    "* conll_03/eng.train\n",
    "\n",
    "If you are unfamiliar with the basics of Text Extensions for Pandas, we recommend you \n",
    "start with Part 1 of this example.\n",
    "\n",
    "### Introduction\n",
    "\n",
    "At the end of part 2 of the demo, we've shown that there are incorrect labels hidden in the CoNLL-2003 validation set, and that you can pinpoint those incorrect labels by data-mining the results of the 16 models the competitors submitted.\n",
    "\n",
    "Our goal for part 3 of the demo is to pinpoint incorrect labels across the entire data set. The (rough) process to do so will be:\n",
    "\n",
    "1. Retokenize the entire corpus using a \"BERT-compatible\" tokenizer, and map the token/entity labels from the original corpus on to the new tokenization.\n",
    "2. Generate BERT embeddings for every token in the entire corpus in one pass, and store those embeddings in a dataframe column (of type TensorType) alongside the tokens and labels.\n",
    "3. Use the embeddings to quickly train multiple models at multiple levels of sophistication (something like: SVMs, random forests, and LSTMs with small and large numbers of hidden states). Split the corpus into 10 parts and perform a 10-fold cross-validation.\n",
    "4. Repeat the process from part 2 on each fold of the 10-fold cross-validation, comparing the outputs of every model on the validation set for each fold.\n",
    "5. ?\n",
    "6. Profit!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZATION BOILERPLATE\n",
    "\n",
    "# The Jupyter kernel for this notebook usually starts up inside the notebooks\n",
    "# directory, but the text_extensions_for_pandas package code is in the parent\n",
    "# directory. Add that parent directory to the front of the Python include path.\n",
    "import sys\n",
    "if (sys.path[0] != \"..\"):\n",
    "    sys.path[0] = \"..\"\n",
    "    \n",
    "# Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# And of course we need the text_extensions_for_pandas library itself.\n",
    "import text_extensions_for_pandas as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the corpus in its original tokenization\n",
    "gold_standard = tp.conll_2003_to_dataframes(\"../conll_03/eng.testa\")\n",
    "gold_standard_spans = [tp.iob_to_spans(df) for df in gold_standard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_standard[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bert_tokens(target_text: str, \n",
    "                     tokenizer: BertTokenizerFast # TODO: More general type?\n",
    "                    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Tokenize the indicated text for BERT embeddings and return a dataframe\n",
    "    with one row per token.\n",
    "    \n",
    "    :param: target_text\n",
    "    \n",
    "    :returns: A `pd.DataFrame` with the following columns:\n",
    "     * \"id\": unique integer ID for each token\n",
    "     * \"char_span\": span of the token with character offsets\n",
    "     * \"token_span\": span of the token with token offsets\n",
    "     * \"input_id\": integer ID suitable for input to a BERT embedding model\n",
    "     * \"token_type_id\": TODO: Accurate description of this column\n",
    "     * \"attention_mask\": TODO: Accurate description of this column\n",
    "     * \"special_tokens_mask\": `True` if the token is a zero-length special token\n",
    "       such as \"start of document\"\n",
    "    \"\"\"\n",
    "    tokenized_result = tokenizer.encode_plus(target_text, \n",
    "                                             return_special_tokens_mask=True, \n",
    "                                             return_offsets_mapping=True)\n",
    "    df = pd.DataFrame.from_dict(tokenized_result)\n",
    "    # Get offset mapping from tokenizer\n",
    "    offsets = tokenized_result[\"offset_mapping\"]\n",
    "\n",
    "    # Turn special tokens into zero-length spans\n",
    "    begins = np.zeros(len(df.index), dtype=np.int64)\n",
    "    ends = np.zeros(len(df.index), dtype=np.int64)\n",
    "    prev_begin = 0\n",
    "    for i in range(len(df.index)):\n",
    "        if offsets[i] is None:\n",
    "            # Special token --> zero length span\n",
    "            begins[i] = prev_begin\n",
    "            ends[i] = prev_begin\n",
    "        else:\n",
    "            begins[i], ends[i] = offsets[i]\n",
    "            prev_begin = begins[i]\n",
    "  \n",
    "    char_spans = tp.CharSpanArray(target_text, begins, ends)\n",
    "    df[\"char_span\"] = char_spans\n",
    "    df[\"token_span\"] = tp.TokenSpanArray(char_spans, list(range(len(char_spans))), \n",
    "                                         list(range(1, len(char_spans) + 1)))\n",
    "    \n",
    "    # Reformat to look more like the output of make_tokens_and_features()\n",
    "    token_features = pd.DataFrame({\n",
    "        \"id\": df.index,\n",
    "        # Use values instead of series because different indexes\n",
    "        \"char_span\": df[\"char_span\"].values,\n",
    "        \"token_span\": df[\"token_span\"].values,\n",
    "        \"input_id\": df[\"input_ids\"].values,\n",
    "        \"token_type_id\": df[\"token_type_ids\"].values,\n",
    "        \"attention_mask\": df[\"attention_mask\"].values,\n",
    "        \"special_tokens_mask\": df[\"special_tokens_mask\"].values.astype(np.bool)\n",
    "    })\n",
    "    return token_features\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', add_special_tokens=True)\n",
    "#token_features = make_bert_tokens(\"I have a puppy.\", tokenizer)\n",
    "#token_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retokenize the corpus's text with the BERT tokenizer\n",
    "bert_toks = [make_bert_tokens(df[\"char_span\"].values[0].target_text, tokenizer) for \n",
    "             df in gold_standard]\n",
    "bert_toks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_toks = bert_toks[0]\n",
    "doc_toks[doc_toks[\"special_tokens_mask\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align the BERT tokens with the original tokenization and regenarate the IOB tags\n",
    "# Start by converting the elements of gold_standard_spans to character spans\n",
    "new_gold_standard_spans = []\n",
    "for i in range(len(gold_standard_spans)):\n",
    "    #print(f\"document {i}\")\n",
    "    g = gold_standard_spans[i]\n",
    "    char_spans = tp.CharSpanArray._from_sequence(g[\"token_span\"].values)\n",
    "    bert_token_spans = tp.TokenSpanArray.align_to_tokens(bert_toks[i][\"char_span\"],\n",
    "                                                         char_spans)\n",
    "    \n",
    "    #print(f\"char_spans:\\n{char_spans}\")\n",
    "    #print(f\"bert_token_spans:\\n{bert_token_spans}\")\n",
    "    \n",
    "    new_gold_standard_spans.append(pd.DataFrame({\n",
    "        \"char_span\": char_spans,\n",
    "        \"token_span\": g[\"token_span\"].values,\n",
    "        \"bert_token_span\": bert_token_spans\n",
    "    }))                                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_gold_standard_spans[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# * Move the functions above into text_extensions_for_pandas package\n",
    "# * Add regression tests of the new functions\n",
    "# * Add regression test of align_to_tokens\n",
    "# * Implement the inverse of iob_to_spans(), say, \"spans_to_iob\"\n",
    "# * In this notebook, use spans_to_iob() to add columns \"ent_iob\" and \"ent_type\"\n",
    "#   to the BERT tokens dataframe for each document\n",
    "# * Also in this notebook, align the sentences on BERT tokens and add a \n",
    "#   \"sentence\" column to the BERT tokens dataframe for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
