{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoNLL-2003 Example for Text Extensions for Pandas\n",
    "### Part 2\n",
    "\n",
    "To run this notebook, you will need to obtain a copy of the CoNLL-2003 data set's corpus.\n",
    "Drop the corpus's files into the following locations:\n",
    "* conll_03/eng.testa\n",
    "* conll_03/eng.testb\n",
    "* conll_03/eng.train\n",
    "\n",
    "If you are unfamiliar with the basics of Text Extensions for Pandas, we recommend you \n",
    "start with Part 1 of this example.\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In Part 1 of this demo, we showed how to use Text Extensions for Pandas to examine the \n",
    "overall result quality of one entrant in the CoNLL-2003 Shared Task, as well as how to\n",
    "identify and examine the documents from the validation set where that entry had the \n",
    "most errors.\n",
    "\n",
    "In Part 2, we'll perform a broader analysis that goes across all the contents entries\n",
    "and come to some deeper and more surprising conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZATION BOILERPLATE\n",
    "\n",
    "# The Jupyter kernel for this notebook usually starts up inside the notebooks\n",
    "# directory, but the text_extensions_for_pandas package code is in the parent\n",
    "# directory. Add that parent directory to the front of the Python include path.\n",
    "import sys\n",
    "if (sys.path[0] != \"..\"):\n",
    "    sys.path[0] = \"..\"\n",
    "    \n",
    "# Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# And of course we need the text_extensions_for_pandas library itself.\n",
    "import text_extensions_for_pandas as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up the same gold standard data we used in Part 1.\n",
    "gold_standard = tp.conll_2003_to_dataframes(\"../conll_03/eng.testa\")\n",
    "gold_standard_spans = [tp.iob_to_spans(df) for df in gold_standard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up the results from all 16 teams at once.\n",
    "teams = [\"bender\", \"carrerasa\", \"carrerasb\", \"chieu\", \"curran\",\n",
    "         \"demeulder\", \"florian\", \"hammerton\", \"hendrickx\",\n",
    "         \"klein\", \"mayfield\", \"mccallum\", \"munro\", \"whitelaw\",\n",
    "         \"wu\", \"zhang\"]\n",
    "\n",
    "# Read all the output files into one dataframe per <document, team> pair.\n",
    "outputs = { \n",
    "    t: tp.conll_2003_output_to_dataframes(\n",
    "        gold_standard, f\"../resources/conll_03/ner/results/{t}/eng.testb\")\n",
    "    for t in teams\n",
    "}  # Type: Dict[str, List[pd.DataFrame]]\n",
    "\n",
    "# As an example of what we just loaded, show the token metadata for the \n",
    "# \"mayfield\" team's model's output on document 3.\n",
    "outputs[\"mayfield\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results from IOB tags to spans across all teams and documents\n",
    "output_spans = {\n",
    "    t: [tp.iob_to_spans(df) for df in outputs[t]] for t in teams\n",
    "}  # Type: Dict[str, List[pd.DataFrame]]\n",
    "\n",
    "# As an example, show the first 10 spans that the \"florian\" team's model\n",
    "# found on document 2.\n",
    "output_spans[\"florian\"][2].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pandas merge to find what spans match up exactly for each team's\n",
    "# results.\n",
    "# Unlike Part 1, we perform the join across all entity types, looking for\n",
    "# matches of both the extracted span *and* the entity type label.\n",
    "def make_stats_df(gold_dfs, output_dfs):\n",
    "    num_true_positives = [len(gold_dfs[i].merge(output_dfs[i]).index)\n",
    "                          for i in range(len(gold_dfs))]\n",
    "    num_extracted = [len(df.index) for df in output_dfs]\n",
    "    num_entities = [len(df.index) for df in gold_dfs]\n",
    "    doc_num = np.arange(len(gold_dfs))\n",
    "\n",
    "    stats_by_doc = pd.DataFrame({\n",
    "        \"doc_num\": doc_num,\n",
    "        \"num_true_positives\": num_true_positives,\n",
    "        \"num_extracted\": num_extracted,\n",
    "        \"num_entities\": num_entities\n",
    "    })\n",
    "    stats_by_doc[\"precision\"] = stats_by_doc[\"num_true_positives\"] / stats_by_doc[\"num_extracted\"]\n",
    "    stats_by_doc[\"recall\"] = stats_by_doc[\"num_true_positives\"] / stats_by_doc[\"num_entities\"]\n",
    "    stats_by_doc[\"F1\"] = 2.0 * (stats_by_doc[\"precision\"] * stats_by_doc[\"recall\"]) / (stats_by_doc[\"precision\"] + stats_by_doc[\"recall\"])\n",
    "    return stats_by_doc\n",
    "\n",
    "stats = {t: make_stats_df(gold_standard_spans, output_spans[t]) for t in teams}\n",
    "\n",
    "# Show the result quality statistics by document for the \"carrerasa\" team\n",
    "stats[\"carrerasa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 for document 4 is looking a bit low. Is that just fluke, or is it\n",
    "# part of a larger trend? \n",
    "# In Part 1, we showed how to drill down to and examine \"problem\" documents.\n",
    "# Since we have all this additional data, let's try a broader, more \n",
    "# quantitative approach. We'll start by building up some more fine-grained \n",
    "# data about congruence between the gold standard and the model outputs.\n",
    "# Pandas' outer join will tell us what entities showed up just in the gold\n",
    "# standard, just in the model output, or in both sets.\n",
    "# For starters, let's do this just for the \"carrerasa\" team and document 4.\n",
    "gold_standard_spans[4].merge(output_spans[\"carrerasa\"][4], how=\"outer\", indicator=True).sort_values(\"token_span\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the analysis from the previous cell across all teams and documents.\n",
    "# That is, perform an outer join between the gold standard spans dataframe\n",
    "# for each document and the corresponding dataframe from each team.\n",
    "def merge_span_sets(team):\n",
    "    result = []\n",
    "    for i in range(len(gold_standard_spans)):\n",
    "        merged = gold_standard_spans[i].merge(output_spans[team][i], how=\"outer\", indicator=True)\n",
    "        merged[\"gold\"] = merged[\"_merge\"].isin((\"both\", \"left_only\"))\n",
    "        merged[team] = merged[\"_merge\"].isin((\"both\", \"right_only\"))\n",
    "        result.append(merged[[\"token_span\", \"ent_type\", \"gold\", team]])\n",
    "    return result\n",
    "\n",
    "span_flags = {t: merge_span_sets(t) for t in teams}  # Type: Dict[str, List[pd.DataFrame]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have indicator variables for every extracted span, telling whether \n",
    "# it was in the gold standard data set and/or in each of the team's results.\n",
    "# For example, here are the first 5 spans for document 2 in the \"carrerasa\"\n",
    "# team's results:\n",
    "span_flags[\"carrerasa\"][2].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do an n-way merge of all those indicator variables across documents.\n",
    "# This operation produces a single summary dataframe per document.\n",
    "indicators = []  # Type: List[pd.DataFrame]\n",
    "for i in range(len(gold_standard_spans)):\n",
    "    result = gold_standard_spans[i]\n",
    "    for t in teams:\n",
    "        result = result.merge(span_flags[t][i], how=\"outer\")\n",
    "    indicators.append(result.fillna(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have a vector of indicator variables for every span extracted \n",
    "# from every document across all the model outputs and the gold standard.\n",
    "# For example, let's show the results for document 10:\n",
    "indicators[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you look at the above dataframe, you can see that some entities \n",
    "# (\"RUGBY UNION\", for example) are \"easy\", in that almost every entry\n",
    "# found them correctly. Other entities, like \"CAMPESE\", are \"harder\",\n",
    "# in that few of the entrants correctly identified them. Let's add\n",
    "# a column that quantifies this \"difficulty level\" by counting how \n",
    "# many teams found each true or false positive.\n",
    "for df in indicators:\n",
    "    # Convert the teams' indicator columns into a single matrix of \n",
    "    # Boolean values.\n",
    "    vectors = df[df.columns[3:]].values\n",
    "    counts = np.count_nonzero(vectors, axis=1)\n",
    "    df[\"num_teams\"] = counts\n",
    "\n",
    "# Show the dataframe for document 10 again, this time with the new\n",
    "# \"num_teams\" column at the far right.\n",
    "indicators[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can rank the entities in document 10 by \"difficulty\", either as \n",
    "# true positives for the models to find...\n",
    "# (just for document 10 for the moment)\n",
    "ind = indicators[10].copy()\n",
    "ind[ind[\"gold\"] == True].sort_values(\"num_teams\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...or as false positives to avoid:\n",
    "ind[ind[\"gold\"] == False].sort_values(\"num_teams\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get a better picture of what entities are \"difficult\", we need to look \n",
    "# across the entire validation set. Let's combine the dataframes in \n",
    "# `indicators` into a single dataframe that covers all the documents.\n",
    "\n",
    "# First we preprocess each dataframe to make it easier to combine.\n",
    "to_stack = [\n",
    "    pd.DataFrame({\n",
    "        \"doc_id\": i,\n",
    "        # TokenSpanArrays from different documents can't currently be stacked,\n",
    "        # so convert to TokenSpan objects.\n",
    "        \"token_span\" : indicators[i][\"token_span\"].astype(object),\n",
    "        \"ent_type\": indicators[i][\"ent_type\"],\n",
    "        \"gold\": indicators[i][\"gold\"],\n",
    "        \"num_teams\": indicators[i][\"num_teams\"]\n",
    "    })\n",
    "    for i in range(len(indicators))\n",
    "]\n",
    "\n",
    "# Then we concatenate all the preprocessed dataframes into a single dataframe.\n",
    "all_counts = pd.concat(to_stack)\n",
    "\n",
    "all_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can pull out the most difficult entities across the entire validation\n",
    "# set.\n",
    "# First, let's find the most difficult entities from the standpoint of recall:\n",
    "# entities that are in the gold standard, but not in most results.\n",
    "difficult_recall = all_counts[all_counts[\"gold\"] == True].sort_values(\"num_teams\").reset_index(drop=True)\n",
    "difficult_recall.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hmm, everything is zero. How many entities were found by zero teams?  One team?\n",
    "(all_counts[all_counts[\"gold\"] == True][[\"num_teams\", \"token_span\"]]\n",
    " .groupby(\"num_teams\").count()\n",
    " .rename({\"token_span\": \"count\"}))\n",
    " # TODO: The last step here has no effect, due to a bug in Pandas. Fix the bug!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yikes! 140 entities in the validation set were so hard to find, they\n",
    "# were extracted by 0 teams.\n",
    "# Let's go back and look at some of those 0-team entities in context:\n",
    "difficult_recall[\"context\"] = difficult_recall[\"token_span\"].apply(lambda t: t.context())\n",
    "pd.set_option('max_colwidth', 100)\n",
    "difficult_recall.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some of these entities are \"difficult\" because the validation set contains incorrect labels.**\n",
    "\n",
    "For reference, there's a copy of the CoNLL labeling rules in this repository at\n",
    "[resources/conll_03/ner/annotation.txt](../resources/conll_03/ner/annotation.txt)\n",
    "\n",
    "There are 4 incorrect labels in this first set of 20:\n",
    "* `[3289, 3299): 'Full Light'` should be \"Zywiec Full Light\"\n",
    "* `[11, 19): 'Honda RV'` should be tagged `ORG`\n",
    "* `[1525, 1541): 'Consumer Project'` should be \"Consumer Project on Technology\" and should be tagged `ORG`\n",
    "* `[244, 255): 'McDonald 's'` should be tagged `MISC` (because it's an \"adjective ... derived from a word which is ... organisation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the entities that are difficult from the perspective of \n",
    "# precision: that is, in many models' results, but not in the gold standard.\n",
    "difficult_precision = all_counts[all_counts[\"gold\"] == False].sort_values(\"num_teams\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Again, we can add some context to these spans:\n",
    "difficult_precision[\"context\"] = difficult_precision[\"token_span\"].apply(lambda t: t.context())\n",
    "difficult_precision.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As with the entities in `difficult_recall`, some of these entities in `difficult_precision` are \"difficult\" because the validation set has missing and incorrect labels.**\n",
    "\n",
    "**13** of these first 20 \"incorrect\" results are due to missing and incorrect labels:\n",
    "* `[25, 32): 'BRITISH''` in document 202 should be tagged `MISC`.\n",
    "* `[1317, 1327): 'Portsmouth'` in document 207 should be tagged `ORG`, not `LOC`.\n",
    "* `[110, 118): 'Scottish'` in document 199 should be tagged `MISC`\n",
    "  (or `[28, 53): 'SCOTTISH PREMIER DIVISION'` and \n",
    "  `[110, 135): 'Scottish premier division'` should both be tagged `ORG`).\n",
    "* `[146, 163): 'Santiago Bernabeu'` in document 40 should be tagged `MISC`\n",
    "  (because the \"s\" in `[146, 171): 'Santiago Bernabeu stadium'` is not capitalized).\n",
    "* `[239, 251): 'Philadelphia'` in document 223 should be tagged `ORG`, not `LOC`.\n",
    "* `[367, 376): 'Karlsruhe'` in document 36 should be tagged `ORG`, not `LOC`.\n",
    "* `[1003, 1011): 'Congress'` in document 100 should be tagged `ORG`\n",
    "  (also, `[957, 964): 'Chilean' ==> MISC` should be replaced with \n",
    "  `[957, 973): 'Chilean Congress' ==> ORG`).\n",
    "* `[420, 428): 'Freiburg'` in document 36 should be tagged `ORG`, not `LOC`.\n",
    "* In document 70, `[186, 211): 'New York Commodities Desk'`, not `[186, 206): 'New York Commodities'`, should be tagged `ORG`.\n",
    "* `[263, 271): 'St Louis'` in document 223 should be tagged `ORG`, not `LOC`.\n",
    "* `[788, 795): 'Antwerp'` in document 155 should be tagged `LOC`, not `ORG`.\n",
    "* In document 112, `[178, 191): 'John Mills Jr'`, not `[178, 188): 'John Mills'`, should be tagged `PER`.\n",
    "* `[274, 282): 'COLORADO'` in document 223 should be tagged `ORG`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the gold standard data for document 155, for example.\n",
    "# Note line 12.\n",
    "gold_standard_spans[155][0:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above gold standard spans in context. \n",
    "gold_standard_spans[155][\"token_span\"].values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
