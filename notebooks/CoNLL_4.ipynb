{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoNLL-2003 Example for Text Extensions for Pandas\n",
    "### Part 4\n",
    "\n",
    "To run this notebook, you will need to obtain a copy of the CoNLL-2003 data set's corpus.\n",
    "Drop the corpus's files into the following locations:\n",
    "* conll_03/eng.testa\n",
    "* conll_03/eng.testb\n",
    "* conll_03/eng.train\n",
    "\n",
    "If you are unfamiliar with the basics of Text Extensions for Pandas, we recommend you \n",
    "start with Part 1 of this example.\n",
    "\n",
    "### Introduction\n",
    "\n",
    "At the end of part 3 of the demo, we've shown that we can train multiple synthetic models with different levels of deliberate imprecision. We used the ensemble of models to pinpoint incorrect labels in the validation set, using \n",
    "the same methods that we employed in [`CoNLL_2.ipynb`](./CoNLL_2.ipynb).\n",
    "\n",
    "Now we need to pinpoint incorrect labels across the entire data set, including train, test, and validation sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Jupyter kernel for this notebook usually starts up inside the notebooks\n",
    "# directory, but the text_extensions_for_pandas package code is in the parent\n",
    "# directory. Add that parent directory to the front of the Python include path.\n",
    "import sys\n",
    "if (sys.path[0] != \"..\"):\n",
    "    sys.path[0] = \"..\"\n",
    "    \n",
    "# Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "from typing import *\n",
    "import sklearn.model_selection\n",
    "import sklearn.pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# And of course we need the text_extensions_for_pandas library itself.\n",
    "import text_extensions_for_pandas as tp\n",
    "\n",
    "# Common code shared across notebooks comes from util.py\n",
    "import util\n",
    "\n",
    "# BERT Configuration\n",
    "# Keep this in sync with `CoNLL_3.ipynb`.\n",
    "#bert_model_name = \"bert-base-uncased\"\n",
    "#bert_model_name = \"bert-large-uncased\"\n",
    "bert_model_name = \"dslim/bert-base-NER\"\n",
    "tokenizer = transformers.BertTokenizerFast.from_pretrained(bert_model_name, \n",
    "                                                           add_special_tokens=True)\n",
    "bert = transformers.BertModel.from_pretrained(bert_model_name)\n",
    "\n",
    "# Create a Pandas categorical type for consistent encoding of categories\n",
    "# across all documents.\n",
    "_ENTITY_TYPES = [\"LOC\", \"MISC\", \"ORG\", \"PER\"]\n",
    "token_class_dtype, int_to_label, label_to_int = tp.make_iob_tag_categories(_ENTITY_TYPES)\n",
    "\n",
    "# Parameters for splitting the corpus into folds\n",
    "_KFOLD_RANDOM_SEED = 42\n",
    "_KFOLD_NUM_FOLDS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read inputs\n",
    "\n",
    "Read in the corpus, retokenize it with the BERT tokenizer, add BERT embeddings, and convert\n",
    "to a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The raw dataset in its original tokenization\n",
    "raw_data = {\n",
    "    \"valid\": tp.conll_2003_to_dataframes(\"../conll_03/eng.testa\"),\n",
    "    \"test\": tp.conll_2003_to_dataframes(\"../conll_03/eng.testb\"),\n",
    "    \"train\": tp.conll_2003_to_dataframes(\"../conll_03/eng.train\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The three folds of the dataset with BERT tokens and embeddings\n",
    "bert_data = {\n",
    "    key: util.run_with_progress_bar(\n",
    "        len(val), \n",
    "        lambda i: util.conll_to_bert(val[i], tokenizer, bert, token_class_dtype))\n",
    "    for key, val in raw_data.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single dataframe of annotated tokens for the entire corpus and index\n",
    "corpus_df = util.combine_folds(bert_data)\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare folds for a 10-fold cross-validation\n",
    "\n",
    "We divide the documents of the corpus into 10 random samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDs for each of the keys\n",
    "doc_keys = corpus_df[[\"fold\", \"doc_num\"]].drop_duplicates().reset_index(drop=True)\n",
    "doc_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to split the documents randomly into _NUM_FOLDS sets, then\n",
    "# for each stage of cross-validation train a model on the union of\n",
    "# (_NUM_FOLDS - 1) of them while testing on the remaining fold.\n",
    "# sklearn.model_selection doesn't implement this approach directly,\n",
    "# but we can piece it together with some help from Numpy.\n",
    "#from numpy.random import default_rng\n",
    "rng = np.random.default_rng(seed=_KFOLD_RANDOM_SEED)\n",
    "iloc_order = rng.permutation(len(doc_keys.index))\n",
    "kf = sklearn.model_selection.KFold(n_splits=_KFOLD_NUM_FOLDS)\n",
    "\n",
    "train_keys = []\n",
    "test_keys = []\n",
    "for train_ix, test_ix in kf.split(iloc_order):\n",
    "    # sklearn.model_selection.KFold gives us a partitioning of the\n",
    "    # numbers from 0 to len(iloc_order). Use that partitioning to \n",
    "    # choose elements from iloc_order, then use those elements to \n",
    "    # index into doc_keys.\n",
    "    train_iloc = iloc_order[train_ix]\n",
    "    test_iloc = iloc_order[test_ix]\n",
    "    train_keys.append(doc_keys.iloc[train_iloc])\n",
    "    test_keys.append(doc_keys.iloc[test_iloc])\n",
    "\n",
    "train_keys[1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
